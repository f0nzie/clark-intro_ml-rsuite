[
["index.html", "Machine Learning", " Machine Learning Michael Clark https://m-clark.github.io/ 2019-06-17 "],
["preface.html", "Preface", " Preface The purpose of this document is to provide a conceptual introduction to statistical or machine learning (ML) techniques for those that would not normally be exposed to such approaches during their typical required statistical training. Machine learning1 can be described as a form of statistical analysis, often even utilizing well-known and familiar techniques, that has bit of a different focus than traditional analytical practice in applied disciplines. The key notion is that flexible, automatic approaches are used to detect patterns within the data, with a primary focus on making predictions on future data. If one surveys the number of techniques available in ML without any context, one can easily be overwhelmed in terms of the sheer number of approaches, as well as the various tweaks and variations of them. However, the specifics of the techniques are not as important as more general concepts that would be applicable in most every ML setting, and indeed, many traditional techniques as well. While there will be examples using the R statistical environment and descriptions of a few specific approaches, the focus here is more on ideas than application2 and kept at the conceptual level as much as possible. Note that Python versions of the model examples are available here. In addition, Marcio Mourao has provided additional Python examples. As for prerequisite knowledge, I will assume a basic familiarity with regression analyses typically presented in applied disciplines. Regarding programming, none is really required to follow most of the content here, but some knowledge of R would be helpful if one wants to do the examples. Note that I won’t do much explaining of the code, as I will be more concerned with getting to a result than clearly detailing the path to it. Armed with even introductory knowledge, if there are parts of R code that are unclear, there are many resources to investigate and discover the details on one’s own, which results in more learning anyway. The latest version of this document is dated 2019-06-17. It was originally written in March 2013, when there was much more need for it in academia, at least outside of CS and Engineering departments. For all the buzz of machine learning, deep learning, AI, etc., that need still exists, as most applied disciplines still do not teach such techniques as a core part of their training (yet). However, the number of tools and information, within the realm of R and Python especially, has exploded, so you’ll have plenty to keep you occupied. Indeed, I’m not sure how much of the specific techniques and tools demonstrated in this document will be viable in another five years, so be prepared to keep up. Color coding in text: emphasis package function object/class link Also referred to as applied statistical learning, statistical engineering, data science or data mining in various academic, professional, and other contexts (correctly or not).↩ Indeed, there is evidence that with large enough samples many techniques converge to similar performance.↩ "],
["intro.html", "Introduction Explanation &amp; Prediction Terminology Supervised vs. Unsupervised Tools you already have", " Introduction Explanation &amp; Prediction For any particular analysis conducted, emphasis can be placed on understanding the underlying mechanisms which have specific theoretical underpinnings, versus a focus that dwells more on performance and, more to the point, future performance. These are not mutually exclusive goals in the least, and probably most studies contain a little of both in some form or fashion. I will refer to the former emphasis as that of explanation, and the latter that of prediction3. In studies with a more explanatory focus, traditional analysis concerns a single data set. For example, one assumes a data generating distribution for the response, and one evaluates the overall fit of a single model to the data at hand, e.g. in terms of R-squared, and statistical significance for the various predictors in the model. One assesses how well the model lines up with the theory that led to the analysis, and modifies it accordingly, if need be, for future studies to consider. Some studies may look at predictions for specific, possibly hypothetical values of the predictors, or examine the particular nature of covariate effects. In many cases, only a single model is considered. In general though, little attempt is made to explicitly understand how well the model will do with future data, but we hope to have gained greater insight as to the underlying mechanisms guiding the response of interest. Following Breiman (2001), this would be more akin to the data modeling culture. For studies focused on prediction, other techniques are available that are far more focused on performance, not only for the current data under examination, but for future data the selected model might be applied to. While still possible, relative predictor importance is less of an issue, and oftentimes there may be no particular theory to drive the analysis. There may be thousands of input variables and thousands of parameters to estimate, such that no simple summary would likely be possible anyway. However, many of the techniques applied in such analyses are quite powerful, and steps are taken to ensure better results for new data. Referencing Breiman again, this perspective is more of the algorithmic modeling culture. While the two approaches are not exclusive, I present two extreme (though thankfully dated) views of the situation4: To paraphrase provocatively, ‘machine learning is statistics minus any checking of models and assumptions’. \\(\\sim\\) Brian Ripley, 2004 … the focus in the statistical community on data models has: Led to irrelevant theory and questionable scientific conclusions. Kept statisticians from using more suitable algorithmic models. Prevented statisticians from working on exciting new problems. \\(\\sim\\) Leo Breiman, 2001 Respective departments of computer science, statistics, and related fields, now overlap more than ever, as more relaxed views prevail today. However, there are potential drawbacks to placing too much emphasis on explanation or prediction. Performant models that ‘just work’ have the potential to be dangerous if they are little understood. Conversely, situations for which much time is spent sorting out details for an ill-fitting model suffers the converse problem- a variable amount of understanding coupled with little pragmatism. While this document will focus on more prediction-focused approaches, guidance will be provided with an eye toward their use in situations where the typical explanatory approach would be applied, thereby hopefully shedding some light on a path toward obtaining the best of both worlds. Terminology For those used to statistical concepts such as dependent variables, clustering, and predictors, etc. you will have to get used to some differences in terminology5 such as targets, unsupervised learning, and features etc. It doesn’t take too much to get acclimated, even if it is somewhat annoying when one is first starting out. I won’t be too beholden to either in this paper, and it should be clear from the context what’s being referred to. Initially I will start off mostly with non-ML terms and note in brackets an ML version to help the orientation along. Supervised vs. Unsupervised At least one distinction should be made from the outset, supervised vs. unsupervised learning. Supervised learning is the typical regression or classification situation where we are trying to predict some target variable of interest, possibly several. Unsupervised learning seeks patterns within the data without regard to any specific target variable, and would include things like cluster analysis, principal components analysis, etc. They can also be used in conjunction. The focus of this document will be almost entirely on supervised learning, though we will discuss some forms of unsupervised learning by the end. Tools you already have One thing that is important to keep in mind as you begin is that standard techniques from traditional statistics are still available in ML, although even then we might tweak them or do more with them. So, having a basic background in statistics is all that is required to get started with machine learning. Again, the difference between ML and traditional statistical analysis is more of focus than method. The Standard Linear Model All introductory statistics courses will cover linear regression in great detail, and it certainly can serve as a starting point here. We can describe it as follows in matrix notation in terms of the underlying data generating process: \\[\\mu = X\\beta\\] \\[y \\sim N(\\mu,\\sigma^{2}\\textrm{I})\\] Where \\(y\\) is conditionally a normally distributed response [target] with mean \\(\\mu\\) and constant variance \\(\\sigma^{2}\\). X is a typical model matrix, i.e. a matrix of predictor variables and in which the first column is a vector of 1s for the intercept [bias]6, and \\(\\beta\\) is the vector of coefficients [weights] corresponding to the intercept and predictors in the model matrix. What might be given less focus in applied courses is how often standard regression won’t be the best tool for the job, or even applicable in the form it is presented. Because of this, many applied researchers are still hammering screws with it, even as the explosion of statistical techniques of the past quarter century or more has rendered obsolete many current introductory statistical texts that are written for specific disciplines. Even so, the concepts one gains in learning the standard linear model are generalizable, and even a few modifications of it, while still maintaining the basic design, can render it still very effective in situations where it is appropriate. Typically, in fitting [training] a model we tend to talk about R-squared and statistical significance of the coefficients for a small number of predictors. For our purposes, let the focus instead be on the residual sum of squares7, or error, with an eye towards its reduction and model comparison. In ML, we will not have a situation in which we are only considering one model fit, and so must find one that reduces the sum of the squared errors but without unnecessary complexity and overfitting, concepts we’ll return to later. Furthermore, we will be much more concerned with the model fit on new data that we haven’t seen yet [generalization]. Logistic Regression Logistic regression is often used where the response is categorical in nature, usually with binary outcome in which some event occurs or does not occur [label]. One could still use the standard linear model here, but you could end up with nonsensical predictions that fall outside the 0-1 range regarding the probability of the event occurring, to go along with other shortcomings. Furthermore, it is no more effort nor is any understanding lost in using a logistic regression over the linear probability model. It is also good to keep logistic regression in mind as we discuss other classification approaches later on. Logistic regression is also typically covered in an introduction to statistics for applied disciplines because of the pervasiveness of binary responses, or responses that have been made as such8. Like the standard linear model, just a few modifications can enable one to use it to provide better performance, particularly with new data. The gist is, it is not the case that we have to abandon familiar tools in the move toward a machine learning perspective. Expansions of Those Tools Generalized Linear Models To begin, logistic regression is a generalized linear model assuming a binomial distribution for the response and with a logit link function as follows: \\[\\eta = X\\beta\\] \\[\\eta = g(\\mu)\\] \\[\\mu = g(\\eta)^{-1}\\] \\[y \\sim \\mathrm{Bin}(\\mu, \\mathrm{size}=1)\\] This is the same presentation format as seen with the standard linear model presented before, except now we have a link function \\(g(.)\\) and so are dealing with a transformed response. In the case of the standard linear model, the distribution assumed is the Gaussian and the link function is the identity link, i.e. no transformation is made. The link function used will depend on the analysis performed, and while there is choice in the matter, the distributions used have a typical, or canonical link function9. Generalized linear models expand the standard linear model, which is a special case of generalized linear model, beyond the Gaussian distribution for the response, and allow for better fitting models of categorical, count, and skewed response variables. We also have a counterpart to the residual sum of squares, though we’ll now refer to it as the deviance. Generalized Additive Models Additive models extend the generalized linear model to incorporate nonlinear relationships of predictors to the response. We might note it as follows: \\[\\eta = X\\beta + f(Z)\\] \\[\\mu = g(\\eta)^{-1}\\] \\[y \\sim \\mathrm{family}(\\mu, ...)\\] So we have the generalized linear model, but also smooth functions \\(f(Z)\\) of one or more predictors. More detail can be found in Wood (n.d.) and I provide an introduction here. Things do start to get fuzzy with GAMs. It becomes more difficult to obtain statistical inference for the smooth terms in the model, and the nonlinearity does not always lend itself to easy interpretation. However, really this just means that we have a little more work to get the desired level of understanding. GAMs can be seen as a segue toward more black box/algorithmic techniques. Compared to some of those techniques in machine learning, GAMs are notably more interpretable, though GAMs are perhaps less so than GLMs. Also, part of the estimation process includes regularization and validation in determining the nature of the smooth function, topics of which we will return later. References "],
["concepts.html", "Concepts Loss Functions Regularization Bias-Variance Tradeoff Cross-Validation", " Concepts Given a set of predictor variables \\(X\\) and some target \\(y\\), we look for some function, \\(f(X)\\), to make predictions of y from those input variables. We also need a function to penalize errors in prediction, i.e. a loss function. With a chosen loss function, we then find the model which will minimize loss, generally speaking. We will start with the familiar and note a couple others that might be used. Loss Functions Continuous Outcomes Squared Error The classic loss function for linear models with a continuous numeric response is the squared error loss function, or the residual sum of squares. \\[L(Y, f(X)) = \\sum(y-f(X))^2\\] Everyone who has taken a statistics course is familiar with this ‘least-squares’ approach on some level. Often they are not taught that it is one of many possible approaches. However, the average, or mean squared error is commonly used as a metric of performance (or it’s square root). Absolute Error For an approach that is more robust to extreme observations, we might choose absolute rather than squared error. In this case, predictions are a conditional median rather than a conditional mean. \\[L(Y, f(X)) = \\sum|(y-f(X))|\\] Negative Log-likelihood We can also think of our usual likelihood methods learned in a standard applied statistics course10 as incorporating a loss function that is the negative log-likelihood pertaining to the model of interest. Such methods seek to maximize the likelihood of the data given the parameters. To turn it into a loss function, we simply minimize its negative value. As an example, if we assume a normal distribution for the response we can note the loss function as: \\[L(Y, f(X)) = n\\ln{\\sigma} + \\sum \\frac{1}{2\\sigma^2}(y-f(X))^2\\] In this case it would converge to the same answer as the squared error/least squares solution. R Example The following provides conceptual code that one could use with the optim function in R to find estimates of regression coefficients (\\(\\beta\\) from before) based on minimizing the squared error. X is a design matrix of our predictor variables with the first column a vector of 1s in order to estimate the intercept. y is the continuous variable to be modeled. In the following, the true values for the intercept and other coefficients are (0, -.5, .5). We can then compare the results with the lm function from base R11. sqerrloss = function(beta, X, y){ mu = X %*% beta sum((y-mu)^2) } # data setup set.seed(123) # for reproducibility N = 100 # sample size X = cbind(1, X1=rnorm(N), X2=rnorm(N)) # model matrix: intercept, 2 predictors beta = c(0, -.5, .5) # true coef values y = rnorm(N, X%*%beta, sd=1) # target # results our_func = optim(par=c(0,0,0), # starting values fn=sqerrloss, X=X, y=y, method=&#39;BFGS&#39;) lm_result = lm(y ~ ., data.frame(X[,-1])) # check with lm rbind(optim=c(our_func$par, our_func$value), lm=c(coef(lm_result), loss=sum(resid(lm_result)^2))) (Intercept) X1 X2 loss optim 0.1350654 -0.6331715 0.5238113 87.78187 lm 0.1350654 -0.6331715 0.5238113 87.78187 While lm uses a different approach, they are both going to result in the ‘least-squares’ estimates. Just to be clear, this is an exercise to understand how optimization works, not something you’d ever do for the standard regression setting. However, many ML tools will require you to supply your data as a model matrix, specify target(s), and select both a loss function and optimizer, and even possibly additional options for the latter. Categorical Outcomes Here we’ll also look at some loss functions useful in classification problems. Note that there is not necessary exclusion in loss functions for continuous vs. categorical outcomes12. Generally though, we’ll have different options. Misclassification Probably the most straightforward is misclassification, or 0-1 loss. If we note \\(f\\) as the prediction, and for convenience we assume a [-1,1] response instead of a [0,1] response: \\[L(Y, f(X)) = \\sum I(y\\neq \\mathrm{sign}(f))\\] In the above, \\(I\\) is the indicator function, and so we are simply summing misclassifications. Binomial log-likelihood \\[L(Y, f(X)) = \\sum \\ln(1 + e^{-2yf})\\] The above is in deviance form. If you’re not familiar, deviance can conceptually be thought of as the GLM version of residual variance. This loss is equivalent to binomial log likelihood when \\(y\\) is on the 0-1 scale. Exponential Exponential loss is yet another loss function at our disposal. \\[L(Y, f(X)) = \\sum e^{-yf}\\] Hinge Loss A final loss function to consider, typically used with support vector machines, is the hinge loss function. \\[L(Y, f(X)) = \\max(1-yf, 0)\\] Here negative values of \\(yf\\) are misclassifications, and so correct classifications do not contribute to the loss. We could also note it as \\(\\sum (1-yf)_+\\) , i.e. summing only those positive values of \\(1-yf\\). The following image compares these (from Hastie, Tibshirani, and Friedman (2009), fig. 10.4). As before we are assuming y is {-1,1}, and here f is our prediction, where positive values are classified as 1 and negative values are classified as -1. \\(yf\\), the margin, is akin to the residual from regression, where positive values indicate correct classification. Which of these might work best may be specific to the situation, but the gist is that they penalize negative values (misclassifications) more heavily and increasingly (except for misclassification error, which penalizes all misclassifications equally), with their primary difference in how heavy that penalty is. Regularization It is important to note that a model fit to a single data set might do very well with the data at hand, but then suffer when predicting independent data13. Also, oftentimes we are interested in a ‘best’ subset of predictors among a great many, and in this scenario the estimated coefficients are overly optimistic. This general issue can be improved by shrinking estimates toward zero, such that some of the performance in the initial fit is sacrificed for improvement with regard to prediction. Penalized estimation will provide estimates with some shrinkage, and we can use it with little additional effort with our common procedures. Concretely, let’s apply this to the standard linear model, where we are finding estimates of \\(\\beta\\) that minimize the squared error loss. \\[\\hat\\beta = \\underset{\\beta}{\\mathrm{arg\\, min}} \\sum{(y-X\\beta)^2}\\] In words, we’re finding the coefficients that minimize the sum of the squared residuals. Now we just add a penalty component to the procedure as follows. \\[\\hat\\beta = \\underset{\\beta}{\\mathrm{arg\\, min}} \\sum{(y-X\\beta)^2} + \\lambda\\overset{p}{\\underset{j=1}{\\sum}}{\\left|\\beta_j\\right|}\\] In the above equation, \\(\\lambda\\) is our penalty term14 for which larger values will result in more shrinkage. It’s applied to the \\(L_1\\) or Manhattan norm of the coefficients, \\(\\beta_1,\\beta_2...\\beta_p\\), i.e. not including the intercept \\(\\beta_0\\), and is the sum of their absolute values (commonly referred to as the lasso15). For generalized linear and additive models, we can conceptually express a penalized likelihood as follows: \\[l_p(\\beta) = l(\\beta) - \\lambda\\overset{p}{\\underset{j=1}{\\sum}}{\\left|\\beta_j\\right|}\\] As we are maximizing the likelihood, the penalty is a subtraction, but nothing inherently different is shown. If we are minimizing the negative (log) likelihood, we then add the penalty. This basic idea of adding a penalty term will be applied to all machine learning approaches, but as shown, we can apply such a tool to classical methods to boost prediction performance. It should be noted that we can go about the regularization in different ways. For example, using the squared \\(L_2\\) norm results in what is called (a.k.a. Tikhonov regularization)16, and using a weighted combination of the lasso and ridge penalties gives us elastic net regularization. We’ll see an example of this later. R Example In the following example, we take a look at the lasso approach for a standard linear model. We add the regularization component, with a fixed penalty \\(\\lambda\\) for demonstration purposes17. However, you should insert your own values for \\(\\lambda\\) in the optim line to see how the results are affected. I’ve also increased the number of predictors to 10. # data setup set.seed(123) N = 100 X = cbind(1, matrix(rnorm(N*10), ncol=10)) beta = runif(ncol(X)) y = rnorm(N, X%*%beta, sd=2) sqerrloss_reg = function(beta, X, y, lambda=.5){ mu = X%*%beta # sum((y-mu)^2) + lambda*sum(abs(beta[-1])) # conceptual sum((y-mu)^2) + 2*length(y)*lambda*sum(abs(beta[-1])) # actual for lasso } lm_result = lm(y~., data.frame(X[,-1]) ) regularized_result = optim(par=rep(0, ncol(X)), fn=sqerrloss_reg, X=X, y=y, method=&#39;BFGS&#39;) From the above, we can see in this case that the penalized coefficients have indeed shrunk toward zero. Now we move to testing. Normally both the training and test sets will be a random split of the original set, for this demo we’ll generate the test as we did with the training. # Create test data N_test = 50 X_test = cbind(1, matrix(rnorm(N_test*10), ncol=10)) y_test = rnorm(N_test, X_test%*%beta, sd=2) # fits on training set fits_lm = fitted(lm_result, newdata = data.frame(X)) fits_reg = X%*%regularized_result$par # loss on training set data.frame(lm_train = crossprod(y - fits_lm), regularized_train = crossprod(y - fits_reg)) lm_train regularized_train 1 402.2046 583.3891 # fits on test set fits_lm = predict(lm_result, newdata = data.frame(X_test)) fits_reg = X_test%*%regularized_result$par # loss on test set data.frame(lm_test = crossprod(y_test - fits_lm), regularized_test = crossprod(y_test - fits_reg)) lm_test regularized_test 1 254.0681 232.4942 We can see that the residual sum of squares has increased just a tad for the regularized fit on the training data. On the test data however, the squared error loss is lower. In general, we can add the same sort of penalty to any number of models, such as logistic regression, neural net models, recommender systems etc. The primary goal again is to hopefully increase our ability to generalize the selected model to new data. Note that the estimates produced are in fact biased, but we have decreased the variance with new predictions as a counterbalance, and this brings us to the topic of the next section. Bias-Variance Tradeoff In most of science, we are concerned with reducing uncertainty in our knowledge of some phenomenon. The more we know about the factors involved or related to some outcome of interest, the better we can predict that outcome upon the influx of new information. The initial step is to take the data at hand, and determine how well a model or set of models fit the data in various fashions. In many applications however, this part is also more or less the end of the game as well18. Unfortunately, such an approach, in which we only fit models to one data set, does not give a very good sense of generalization performance, i.e. the performance we would see with new data. While typically not reported, most researchers, if they are spending appropriate time with the data, are actually testing a great many models, among which the ‘best’ is then provided in detail in the end report. Without some generalization performance check however, such performance is overstated when it comes to new data. In the following, consider a standard linear model scenario, e.g. with squared-error loss function and perhaps some regularization, and a data set in which we split the observations in some random fashion into a training set, for initial model fit, and a test set, which will be kept separate and independent, and used to measure generalization performance19. We note training error as the (average) loss over the training set, and test error as the (average) prediction error obtained when a model resulting from the training data is fit to the test data. So, in addition to the previously noted goal of finding the ‘best’ model (model selection), we are interested further in estimating the prediction error with new data (model performance). Bias &amp; Variance We start20 with a true data generating process for some target \\(y\\), expressed as a function of features \\(X\\). We can specify the true model as \\[y = f(X) + \\epsilon\\] where \\(f(x)\\) is the expected value of \\(y\\) given \\(X\\), i.e. \\(f(x) = E(y|X)\\). The expected value of the error, \\(E(\\epsilon)=0\\), has some variance, \\(\\textrm{Var}(\\epsilon) = \\sigma^2_\\epsilon\\). In other words, we are talking about the standard regression model we all know and love. Now we can conceptually think of the expected prediction error at a specific input \\(X = x_*\\) as: \\[\\text{Error}_{x_*} = \\text{Irreducible Error} + \\text{Bias}^2 + \\text{Variance}\\] To better understand this, think of training models over and over, each time with new training data, but testing each model at input \\(x_*\\). The \\(\\text{Error}_{x_*}\\) is the average, or expected value of the prediction error in this scenario, or \\(E[(y - \\hat f(x))^2|X=x_*]\\), with \\(\\hat f\\) our current estimate of the true underlying data generating function \\(f\\). We can note three components to this general notion of prediction error: Irreducible error: The variance of the (new test) target (\\(\\sigma^2_\\epsilon\\)). This is unavoidable, since our \\(y\\) is measured with error. Bias2: the amount the average of our estimate varies from the true (but unknown) value (\\(E(\\hat f) - f\\)). This is often the result of trying to model the complexity of nature with something much simpler that the human brain can understand. While the simpler might make us feel good, it may not work very well. Variance: the amount by which our prediction would change if we had estimated it using a different training data set (\\(Var(\\hat f)\\)). Even with unbiased estimates, we could still see a high mean squared error due to high variance. Slightly more formally, we can present this as follows, with \\(h_*\\) our estimated (hypothesized) value at \\(x_*\\): \\[\\text{Error}_{x_*} = \\text{Var}(\\epsilon) + (\\text{E}[h_*] - f(x_*))^2 + \\text{Var}(h_*)\\] The latter two components make up the mean squared error in our previous demonstration. While they are under our control, they compete with one another such that oftentimes we improve one at the detriment of the other. In other words, bias and variance are not independent. The Tradeoff Outlining a general procedure, we start by noting the prediction error on a training data set with multiple models of varying complexity (e.g. increasing the number of predictor variables, adding polynomial terms, including interactions), and then assess the performance of the chosen models in terms of prediction error on the test set. We then perform the same activity for a total of 100 simulated data sets, for each level of complexity. The results from this process might look like the following, taken from Hastie, Tibshirani, and Friedman (2009). With regard to the training data, we have \\(\\mathrm{error}_{\\mathrm{train}}\\) for one hundred training sets for each level of model complexity. The bold blue line notes this average error over the 100 sets by model complexity, and we can see that more complex models fit the data better. The bold red line is the average test error (\\(\\mathrm{error}_{\\mathrm{test}}\\)) across the 100 test data sets, and it tells a different story. When models get too complex, the test error starts to increase. Ideally we’d like to see low bias and (relatively) low variance, but things are not so easy. One thing we can see clearly is that \\(\\mathrm{error}_{\\mathrm{train}}\\) is not a good estimate of \\(\\mathrm{error}_{\\mathrm{test}}\\), which is now our focus in terms of performance. If we think of the training error as what we would see in typical research where one does everything with a single data set, we are using the same data set to fit the model and assess error. As the model is adapted to that data set specifically, it will be overly optimistic in the estimate of the error, that optimism being the difference between the error rate we see based on the training data versus the average of what we would get with many test data sets. We can think of this as a problem of overfitting to the training data. Models that do not incorporate any regularization or validation process of any kind are likely overfit to the data presented. Generally speaking, the more complex the model, the lower the bias, but the higher the variance, as depicted in the graphic. Specifically however, the situation is more nuanced, where the type of problem (classification with 0-1 loss vs. continuous response with squared error loss21) and technique (a standard linear model vs. regularized fit) will exhibit different bias-variance relationships. Diagnosing Bias-Variance Issues &amp; Possible Solutions The following can serve as a visual summary of the concepts just outlined (figure adapted from Domingos (2012)). Now let’s assume a regularized linear model with a standard data split into training and test sets. We will describe different scenarios with possible solutions. Worst Case Scenario Starting with the worst case scenario, poor models may exhibit high bias and high variance. One thing that will not help this situation (perhaps contrary to intuition) is adding more data. You can’t make a silk purse out of a sow’s ear (usually), and adding more data just gives you a more accurate picture of how awful your model is. One might need to rework the model, e.g. adding new predictors or creating them via interaction terms, polynomials, or other smooth functions as in additive models, or simply collecting better and/or more relevant data. High Variance When variance is a problem, our training error is low while test error is relatively high (overfitting problem). Implementing more shrinkage or other penalization to model complexity may help with the issue. In this case more data may help as well. High Bias With bias issues, our training error is high and test error is not too different from training error (underfitting problem). Adding new predictors/features, e.g. interaction terms, polynomials etc., can help here. Additionally, reducing the penalty parameter \\(\\lambda\\) would also work with even less effort, though generally it should be estimated rather than explicitly set. Here is another visualization to drive the point home. The figure is inspired by Murphy (2012) (figure 6.5) showing the bias-variance trade-off. Sample (left) and average (right) fits of linear regression using a Gaussian radial basis function expansion. The blue line represents the true x-y relationship. The top row shows low variance between one fit and the next (left) but notable bias (right) in that the average fit is off. Compare to the less regularized (high variance, low bias) situation of the bottom row. See the kernlab package for the fitting function used, and the appendix for the code used to produce the graph. Bias-Variance Summary One of the key ideas any applied researcher can take from machine learning concerns the bias-variance trade-off and issues of overfitting in particular. Typical applied practice involves potentially dozens of models fit to the same data set without any validation whatsoever, yet only one or two are actually presented in publication. Many disciplines report nothing but the statistical significance, and yet one can have statistically significant predictors and have predictive capability that is no different from guessing. Furthermore, very complex models are often fit to small data sets, compounding the problem. It is very easy to describe science without ever talking about statistical significance. It is impossible to talk about science without talking about prediction. The bias-variance trade-off is one way to bring the concerns of prediction to the forefront, and any applied researcher can benefit from thinking about its implications22. Cross-Validation As noted in the previous section, in machine learning approaches we are particularly concerned with prediction error on new data. The simplest validation approach would be to split the data available into a training and test set as discussed previously. We estimate the model on the training data, and apply the model to the test data, get the predictions and measure our test error, selecting whichever model results in the least test error. The following displays a hypothetical learning curve from the results of such a process. While the approach is fairly simple, other approaches are more commonly used and result in better estimates of predictive performance23. Adding Another Validation Set One technique that might be utilized for larger data sets, is to split the data into training, validation, and test sets. For example, one might take the original data and create something like a 60-20-20% split to create the needed data sets. The purpose of the initial validation set is to select the optimal model and determine the values of tuning parameters. These are parameters which generally deal with how complex a model one will allow, but for which one would have little inkling as to what they should be set at beforehand. For example, our \\(\\lambda\\) shrinkage parameter in regularized regression would be such a parameter. We first select models/tuning parameters that minimize the validation set error, and once the model is chosen, we then examine test set error performance. In this way performance assessment is still independent of the model development process. K-fold Cross-Validation In many cases we don’t have enough data for such a split, and the split percentages are arbitrary anyway, with results that would be specific to the split chosen. Instead we can take a typical data set and randomly split it into \\(\\kappa=10\\) equal-sized (or close to it) parts. Next, we take the first nine partitions, combine them, and use them as the training set. With chosen model from the training data, make predictions on the held-out partition. Now we do it again, but this time use the 9th partition as the holdout set. Repeat the process until each of the initial 10 partitions of data have been used as the test set. Average the error across all procedures for our estimate of prediction error. With enough data, this (and the following methods) could be used as the validation procedure before eventual performance assessment on an independent test set with the final chosen model. An illustration of 3-fold classification. Leave-one-out Cross-Validation Leave-one-out (LOO) cross-validation is the same thing but where \\(\\kappa=N\\). In other words, we train a model for all observations except the \\(\\kappa^{th}\\) one, assessing fit on the observation that was left out. We then cycle through until all observations have been left out once to obtain an average accuracy. Of the two, K-fold may have relatively higher bias but less variance, while LOO would have the converse problem, as well as possible computational issues24. K-fold’s additional bias would be diminished would with increasing sample sizes, and generally 5 or 10-fold cross-validation is recommended. However, many model selection techniques (e.g. via AIC) have a leave-one-out interpretation. Bootstrap With a bootstrap approach, we draw \\(B\\) random samples with replacement from our original data set, creating \\(B\\) bootstrapped data sets of the same size as the original data. We use the \\(B\\) data sets as training sets and, using the original data as the test set, average the prediction error across the models. Other Stuff Along with the above there are variations such as repeated cross validation, the ‘.632’ bootstrap and so forth. One would want to do a bit of investigating, but \\(\\kappa\\)-fold and bootstrap approaches generally perform well. If variable selection is part of the goal, one should be selecting subsets of predictors as part of the cross-validation process, not at some initial data step. References "],
["blackbox.html", "Opening the Black Box Process Overview The Dataset R Implementation Feature Selection &amp; The Data Partition Regularized Regression \\(k\\)-nearest Neighbors Neural Networks Trees &amp; Forests Support Vector Machines", " Opening the Black Box library(clark.introml.book.pkgs) library(visibly) library(dplyr) library(kableExtra) library(tidyr) library(tibble) library(htmltools) library(tidyext) library(tidyverse) It’s now time to see some machine learning in action. In what follows, a variety of techniques will be used on the same problem to provide a better feel for the sorts of tools that are available. Process Overview Despite the facade of a polished product one finds in published research, most of the approach with the statistical analysis of data is full of data preparation, starts and stops, debugging, re-analysis, tweaking and fine-tuning etc. Statistical learning is no different in this sense. Before we begin with explicit examples, it might be best to give a general overview of the path we’ll take. Data Preparation As with any typical statistical project, probably most of the time will be spent preparing the data for analysis. Data is never ready to analyze right away, and careful checks must be made in order to ensure the integrity of the information. This would include correcting errors of entry, noting extreme values, possibly imputing missing data and so forth. In addition to these typical activities, we will discuss a couple more things to think about during this initial data examination when engaged in machine learning. Define Data and Data Partitions As we have noted previously, ideally we will have enough data to incorporate some validation procedure. This would be some random partition of the data such that we could safely conclude that the data in the test set comes from the same population as the training set. The validation set is used to fit the initial models at various tuning parameter settings, with a ‘best’ model being that which satisfies some criterion. With final model and parameters chosen, generalization error will be assessed with the performance of the chosen model on the test data. In addition, when data is too large to fit in memory, it will possibly need to be partitioned regardless of the modeling scenario. Spark, Hadoop and other frameworks allow for analysis of data that is potentially distributed across several machines or cores within a machine. Feature Scaling Even with standard regression modeling, centering continuous variables (subtracting the mean) is a good idea so that intercepts and zero points are meaningful. Standardizing variables so that they have similar variances or ranges will help some procedures find their maximums/minimums faster. Another common transformation is min-max normalization25, which will transfer a scale to a new one of some chosen minimum and maximum. Note that whatever approach is done, it must be done after any explicit separation of data. So if you have separate training and test sets, they should be scaled separately. Of course, with enough data and appropriate partitioning, it shouldn’t matter. Feature Engineering If we’re lucky we’ll have ideas on potential combinations or other transformations of the predictors we have available. For example, in typical social science research there are two-way interactions one is often predisposed to try, or perhaps one can sum multiple items to a single scale score that may be more relevant. Another common technique is to use a dimension reduction scheme such as principal components, but this can (and probably should) actually be implemented as part of the ML process26. However, many techniques are fine with more predictors than observations, even some still do well if many of them are irrelevant. One can implement a variety of such approaches in ML to create additional potentially relevant features, as well, even automatically. As a reminder though, a key concern is overfitting, and doing broad construction of this sort with no contextual guidance would potentially be prone to such a pitfall. In other cases, it may simply be not worth the time expense. Discretization While there may be some contextual exceptions to the rule, it is generally a pretty bad idea in standard statistical modeling to discretize/categorize continuous variables27. However, some ML procedures will work better (or just notably faster) if dealing with discrete valued predictors rather than continuous. Others even require them; for example, logic regression needs binary input. While one could pick arbitrary intervals and cut-points in an unsupervised fashion, such as picking equal range bins or equal frequency bins, there are supervised approaches that will use the information in the data to produce some ‘optimal’ discretization. It’s generally not a good idea to force things in data analysis, and given that a lot of data situations will contain mixed data types, it seems easier to simply apply some scaling to preserve the inherent relationships in the data. Again though, if one has only a relative few continuous variables, or a context in which it makes sense to, it’s probably better to leave continuous variables as such. Model Selection With data prepared and ready to analyze, one can use a validation process to come up with a viable model. Use an optimization procedure or a simple grid search over a set of specific values to examine models at different tuning parameters. Perhaps make a finer search once an initial range of good performing values is found, though one should not split hairs over arbitrarily close performance. Select a ‘best’ model given some criterion such as overall accuracy, or if concerned about over fitting, select the simplest model within one standard error of the accuracy of the best, or perhaps the simplest within X% of the best model. For highly skewed classes, one might need to use a different measure of performance besides accuracy, as simply guessing the most common category could lead to very high accuracy. If one has a great many predictor variables, one may use the model selection process to select features that are ‘most important’. Model Assessment With tuning parameters/features chosen, we then examine performance on the independent test set (or via some validation procedure). For classification problems, consider other statistics besides accuracy as measures of performance, especially if classes are unbalanced. Consider other analytical techniques that are applicable and compare performance among the different approaches. One can even combine disparate models’ predictions to possibly create an even better classifier28. Regression In typical model comparison within the standard linear model framework, there are a number of ways in which we might assess performance across competing models. For standard OLS regression we might examine adjusted-\\(R^2\\) or a statistical test with nested models, or with the generalized linear models we might pick a model with the lowest AIC29. As we have already discussed, in the machine learning context we are interested in models that reduce e.g. squared error loss (regression) or misclassification error (classification). However, in dealing with many models some differences in performance may be arbitrary. Beyond Classification Accuracy: Other Measures of Performance In typical classification situations we are interested in overall accuracy. There are situations however, not uncommon, in which simple accuracy isn’t a good measure of performance. As an example, consider the prediction of the occurrence of a rare disease. Guessing a non-event every time might result in 99.9% accuracy, but that isn’t how we would prefer to go about assessing some classifier’s performance. To demonstrate other sources of classification information, we will use the following 2x2 table that shows values of some binary outcome (0 = non-event, 1 = event occurs) to the predictions made by some model for that response (arbitrary model). Both a table of actual values, often called a confusion matrix30, and an abstract version are provided. Actual = 1 Actual = 0 Predicted = 1 41 21 Predicted = 0 16 13 Actual = 1 Actual = 0 Predicted = 1 A B Predicted = 0 C D True Positive, False Positive, True Negative, False Negative: Above, these are A, B, D, and C respectively. Accuracy: Number of correct classifications out of all predictions ((A+D)/Total). In the above example this would be (41+13)/91, about 59%. Error Rate: 1 - Accuracy. Sensitivity: is the proportion of correctly predicted positives to all true positive events: A/(A+C). In the above example this would be 41/57, about 72%. High sensitivity would suggest a low type II error rate (see below), or high statistical power. Also known as true positive rate. Specificity: is the proportion of correctly predicted negatives to all true negative events: D/(B+D). In the above example this would be 13/34, about 38%. High specificity would suggest a low type I error rate (see below). Also known as true negative rate. Positive Predictive Value (PPV): proportion of true positives of those that are predicted positives: A/(A+B). In the above example this would be 41/62, about 66%. Negative Predictive Value (NPV): proportion of true negatives of those that are predicted negative: D/(C+D). In the above example this would be 13/29, about 45%. Precision: See PPV. Recall: See sensitivity. Lift: Ratio of positive predictions given actual positives to the proportion of positive predictions out of the total: (A/(A+C))/((A+B)/Total). In the above example this would be (41/(41+16))/((41+21)/(91)), or 1.06. F Score (F1 score): Harmonic mean of precision and recall: 2*(Precision*Recall)/(Precision+Recall). In the above example this would be 2*(.66*.72)/(.66+.72), about 0.69. Type I Error Rate (false positive rate): proportion of true negatives that are incorrectly predicted positive: B/(B+D). In the above example this would be 21/34, about 62%. Also known as alpha. Type II Error Rate (false negative rate): proportion of true positives that are incorrectly predicted negative: C/(C+A). In the above example this would be 16/57, about 28%. Also known as beta. False Discovery Rate: proportion of false positives among all positive predictions: B/(A+B). In the above example this would be 21/62, about 34%. Often used in multiple comparison testing in the context of ANOVA. Phi coefficient: A measure of association: (A*D - B*C)/(sqrt((A+C)*(D+B)*(A+B)*(D+C))). In the above example this would be 0.11. Note the following summary of several measures where \\(N_+\\) and \\(N_-\\) are the total true positive values and total true negative values respectively, and \\(T_+\\), \\(F_+\\), \\(T_-\\) and \\(F_-\\) are true positive, false positive, etc.: Actual = 1 Actual = 0 Predicted = 1 T+/N+ = TPR = sensitivity =recall F+/N- = Type I Predicted = 0 F-/N+ = Type II T-/N+ = TNR = specificity There are many other measures such as area under a Receiver Operating Curve (ROC), odds ratio, and even more names for some of the above. The gist is that given any particular situation you might be interested in one or several of them, and it would generally be a good idea to look at a few. The Dataset We will use the wine data set from the UCI Machine Learning data repository. The goal is to predict wine quality, of which there are 7 values (integers 3-9). We will turn this into a binary classification task to predict whether a wine is ‘good’ or not, which is arbitrarily chosen as 6 or higher. After getting the hang of things, you might redo the analysis as a multiclass problem or even toy with regression approaches, just note there are very few 3s or 9s so you really only have 5 values to work with. The original data along with detailed description can be found here, but aside from quality it contains predictors such as residual sugar, alcohol content, acidity and other characteristics of the wine31. The original data is separated into white and red data sets. I have combined them and created additional variables: color and its binary version white indicating whether it is a white wine, and good, indicating scores greater than or equal to 6 (denoted as ‘Good’, else ‘Bad’). Feel free to inspect the data a little bit before moving on32. wine = read.csv(&#39;data/wine.csv&#39;) R Implementation I will use the caret package in R. It makes implementation of validation, data partitioning, performance assessment, prediction and other procedures about as easy as it can be. However, caret is mostly using other R packages for the modeling functions underlying the process, and those should be investigated for additional information. Check out the caret home page for more detail. The methods selected here were chosen for breadth of approach, in order to give a good sense of the variety of techniques available. In what follows, the associated packages and functions used are: glmnet: glmnet class: knn caret: avNNet randomForest: randomForest e1071: svm In addition to caret, it’s a good idea to use your computer’s resources as much as possible, or some of these procedures may take a notably long time, and more so with the more data you have. The caret package will do this behind the scenes, but you first need to set things up. Say, for example, you have a quad core processor, meaning your processor has four cores essentially acting as independent CPUs. You can set up R for parallel processing, then run caret functions as you normally would. The following code demonstrates how, but see this for details. library(doParallel) cl = makeCluster(7) registerDoParallel(cl) # All subsequent models are then run in parallel model = train(y ~ ., data = training, method = &#39;rf&#39;) Feature Selection &amp; The Data Partition This data set is large enough to leave a holdout sample, allowing us to initially search for the best of a given modeling approach over a grid of tuning parameters specific to the technique. To iterate previous discussion, we don’t want test performance contaminated with the tuning process. With the best model at \\(t\\) tuning parameter(s), we will assess performance with prediction on the holdout set. I also made some decisions to deal with the notable collinearity in the data, which can severely hamper some methods. We can look at the simple correlation matrix to start (hover for values). I ran regressions to examine the R2 for each predictor in a model as if it were the dependent variable predicted by the other input variables. The highest was for density at over 96%33, and further investigation suggested color and sulfur dioxide are largely captured by the other variables already also. These will not be considered in the following models. Caret has its own partitioning function we can use here to separate the data into training and test data. There are 6497 total observations, of which I will put 80% into the training set. The function createDataPartition will produce indices to use as the training set. In addition to this, we will standardize the continuous variables to have a mean of zero and standard deviation of one. For the training data set, this will be done as part of the training process, so that any subsets under consideration are scaled separately, but for the test set caret will do it automatically. library(caret) set.seed(1234) # so that the indices will be the same when re-run trainIndices = createDataPartition(wine$good, p=.8, list=F) wine_train = wine %&gt;% select(-free.sulfur.dioxide, -density, -quality, -color, -white) %&gt;% slice(trainIndices) wine_test = wine %&gt;% select(-free.sulfur.dioxide, -density, -quality, -color, -white) %&gt;% slice(-trainIndices) Let’s take an initial peek at how the predictors separate on the target. In the following I’m ‘predicting’ the pre-possessed data so as to get the transformed data. Again, we’ll leave the preprocessing to the training process eventually, but here it will put them on the same scale for visual display. wine_trainplot = select(wine_train, -good) %&gt;% preProcess(method=&#39;range&#39;) %&gt;% predict(newdata= select(wine_train, -good)) ## featurePlot(wine_trainplot, wine_train$good, &#39;box&#39;) For the training set, it looks like alcohol content34 and volatile acidity separate most with regard to the ‘good’ class. While this might give us some food for thought, note that the figure does not give insight into interaction or nonlinear effects, which some methods we use will explore. Regularized Regression We discussed regularized regression previously, and can start with such models. This allows us to begin with the familiar (logistic) regression model we’re used to, while incorporating concepts and techniques from the machine learning world. In this example, we’ll use the glmnet package, which will allow for a mix of lasso and ridge regression approaches. Thus, we’ll have a penalty based on both absolute and squared values of the coefficients. In this case, if we set the mixing parameter alpha to 1, we have your standard lasso penalty, and 0 would be the ridge penalty. We have another parameter, lambda, which controls the penalty amount. You may be thinking- what should alpha and lambda be set to? They are examples of tuning parameters, for which we have no knowledge about their value without doing some initial digging. As such we will select their values as part of the validation process35. The caret package provides several techniques for validation such as \\(k\\)-fold, bootstrap, leave-one-out and others. We will use 10-fold cross validation. We will also set up a range of values for alpha and lambda. However, note that caret also has ways to search the space automatically. One argument, tuneLength, will create a grid of that number of values for each tuning parameter (e.g. tuneLength=3 for two parameters would create a 9x2 grid as we do below). In addition, there is a search argument for trainControl that can search the parameter space randomly but create no more values than what is specified by tuneLength. cv_opts = trainControl(method=&#39;cv&#39;, number=10) regreg_opts = expand.grid(.alpha = seq(.1, 1, length = 5), .lambda = seq(.1, .5, length = 5)) results_regreg = train(good~., data=wine_train, method = &quot;glmnet&quot;, trControl = cv_opts, preProcess = c(&quot;center&quot;, &quot;scale&quot;), tuneGrid = regreg_opts) glmnet 5199 samples 9 predictor 2 classes: &#39;Bad&#39;, &#39;Good&#39; Pre-processing: centered (9), scaled (9) Resampling: Cross-Validated (10 fold) Summary of sample sizes: 4680, 4679, 4679, 4679, 4679, 4679, ... Resampling results across tuning parameters: alpha lambda Accuracy Kappa 0.100 0.1 0.7268688 0.3528105028 0.100 0.2 0.6935995 0.2373904919 0.100 0.3 0.6720595 0.1487924478 0.100 0.4 0.6497426 0.0644957753 0.100 0.5 0.6368550 0.0159741918 0.325 0.1 0.7151424 0.3159373449 0.325 0.2 0.6701339 0.1370788016 0.325 0.3 0.6330066 0.0002775041 0.325 0.4 0.6330066 0.0000000000 0.325 0.5 0.6330066 0.0000000000 0.550 0.1 0.7037933 0.2757011572 0.550 0.2 0.6330066 0.0000000000 0.550 0.3 0.6330066 0.0000000000 0.550 0.4 0.6330066 0.0000000000 0.550 0.5 0.6330066 0.0000000000 0.775 0.1 0.6891786 0.2192358192 0.775 0.2 0.6330066 0.0000000000 0.775 0.3 0.6330066 0.0000000000 0.775 0.4 0.6330066 0.0000000000 0.775 0.5 0.6330066 0.0000000000 1.000 0.1 0.6620551 0.1130690396 1.000 0.2 0.6330066 0.0000000000 1.000 0.3 0.6330066 0.0000000000 1.000 0.4 0.6330066 0.0000000000 1.000 0.5 0.6330066 0.0000000000 Accuracy was used to select the optimal model using the largest value. The final values used for the model were alpha = 0.1 and lambda = 0.1. The results suggest \\(\\alpha\\) = 0.1 and \\(\\lambda\\) = 0.1. Do not mistake these for truth! However, we have to make a decision, and this at least provides us the means. The additional column, \\(\\kappa\\) (kappa), can be seen as a measure of agreement between predictions and true values, but corrected for chance agreement. It suggests several settings would result in models that do not do any better than chance. Here is visual display of the results. ggplot(results_regreg) ggplot(results_regreg) + theme_trueMinimal() With the tuning parameters in place, now let’s see the results on the test set. The predict function here will automatically use the best \\(\\alpha\\) and \\(\\lambda\\) from the previous process. preds_regreg = predict(results_regreg, wine_test) good_observed = wine_test$good confusionMatrix(preds_regreg, good_observed, positive=&#39;Good&#39;) Confusion Matrix and Statistics Reference Prediction Bad Good Bad 194 73 Good 282 749 Accuracy : 0.7265 95% CI : (0.7014, 0.7506) No Information Rate : 0.6333 P-Value [Acc &gt; NIR] : 6.665e-13 Kappa : 0.3512 Mcnemar&#39;s Test P-Value : &lt; 2.2e-16 Sensitivity : 0.9112 Specificity : 0.4076 Pos Pred Value : 0.7265 Neg Pred Value : 0.7266 Prevalence : 0.6333 Detection Rate : 0.5770 Detection Prevalence : 0.7943 Balanced Accuracy : 0.6594 &#39;Positive&#39; Class : Good We get a lot of information here, but to focus on accuracy, we get around 72.7%. Note that this is performance on the test set, i.e. data that wasn’t used to train the model. The lower bound (and p-value) suggests we are statistically predicting better than the No Information Rate (i.e., just guessing the more prevalent ‘Bad’ category), and sensitivity and positive predictive power are good, though at the cost of being able to distinguish bad wine. The other metrics not previously discussed are as follows: Kappa: a chance-corrected measure of agreement between predictions and true values. Mcnemar’s test: this is actually just Mcnemar’s test on the confusion matrix; like a Chi-square for paired data. Prevalence: how often does the event occur (A+C)/Total Detection Rate: A/Total Detection Prevalence: How often the positive class was predicted (A+B)/Total Balanced Accuracy: The mean of sensitivity and specificity. In addition, one can get the precision/recall form of the output by changing the mode for the confusion matrix. confusionMatrix(preds_regreg, good_observed, positive=&#39;Good&#39;, mode=&#39;prec_recall&#39;) Confusion Matrix and Statistics Reference Prediction Bad Good Bad 194 73 Good 282 749 Accuracy : 0.7265 95% CI : (0.7014, 0.7506) No Information Rate : 0.6333 P-Value [Acc &gt; NIR] : 6.665e-13 Kappa : 0.3512 Mcnemar&#39;s Test P-Value : &lt; 2.2e-16 Precision : 0.7265 Recall : 0.9112 F1 : 0.8084 Prevalence : 0.6333 Detection Rate : 0.5770 Detection Prevalence : 0.7943 Balanced Accuracy : 0.6594 &#39;Positive&#39; Class : Good Perhaps the other approaches that follow will have more success, but note that the caret package does have the means to focus on other metrics during the training process besides accuracy, such as sensitivity, which might help. Also, feature combination or other avenues might help improve the results as well. Additional information from this modeling process reflects the importance of predictors. For most methods accessed by caret, the default variable importance metric regards the area under the curve or AUC from a ROC analysis with regard to each predictor, and is model independent. This is then normalized so that the least important is 0 and most important is 10036. For this approach, i.e. with glmnet, it merely reflects the absolute value of the coefficients, which is okay since we scaled the data as part of the process. Strengths &amp; Weaknesses Strengths Intuitive approach. In the end, it’s still just a standard regression model you’re already familiar with. Widely used for many problems. Would be fine to use in any setting you would use standard/logistic regression. Weaknesses Does not automatically seek out interactions and non-linearity, and as such will generally not be as predictive as other techniques. Variables have to be scaled or results will just be reflective of data types. May have issues with correlated predictors Final Thoughts Incorporating regularization would be fine as your default method, and something to strongly consider. Furthermore, these approaches will have better prediction on new data than their standard complements. As such they are a nice balance between staying interpretable while enhancing predictive capability. However, in general they are not going to be as strong of a method as others in the ML universe, and possibly not even competitive without a lot of feature engineering. If prediction is all you care about for a particular modeling setting, you’ll want to try something else. \\(k\\)-nearest Neighbors Consider the typical distance matrix37 that is often used for cluster analysis of observations38. If we choose something like Euclidean distance as a metric, each point in the matrix gives the value of how far an observation is from some other, given their respective values on a set of variables. K-nearest neighbors approaches exploit this information for predictive purposes. Let us take a classification example, and \\(k = 5\\) neighbors. For a given observation \\(x_i\\), find the 5 closest neighbors in terms of Euclidean distance based on the predictor variables. The class that is predicted is whatever class the majority of the neighbors are labeled as39. For continuous outcomes, we might take the mean of those neighbors as the prediction. In this scenario, the number of neighbors \\(k\\) will be the tuning parameter, and so we will have a set of values for \\(k\\) to try out40. The workhorse function under the hood originally comes from the highly descriptively named class package, authored by the well-known country music duo Venables &amp; Ripley. knn_opts = data.frame(k=c(seq(3, 11, 2), 25, 51, 101)) # odd to avoid ties results_knn = train(good~., data=wine_train, method=&#39;knn&#39;, preProcess=c(&#39;center&#39;, &#39;scale&#39;), trControl=cv_opts, tuneGrid = knn_opts) results_knn k-Nearest Neighbors 5199 samples 9 predictor 2 classes: &#39;Bad&#39;, &#39;Good&#39; Pre-processing: centered (9), scaled (9) Resampling: Cross-Validated (10 fold) Summary of sample sizes: 4680, 4679, 4679, 4679, 4679, 4679, ... Resampling results across tuning parameters: k Accuracy Kappa 3 0.7536051 0.4609387 5 0.7443747 0.4359164 7 0.7505278 0.4479179 9 0.7487971 0.4432620 11 0.7551403 0.4575305 25 0.7528411 0.4504282 51 0.7493703 0.4368867 101 0.7445618 0.4207894 Accuracy was used to select the optimal model using the largest value. The final value used for the model was k = 11. In this case it looks like choosing the nearest 11 neighbors (\\(k=\\) 11) works best in terms of accuracy. Now that \\(k\\) is chosen, let’s see how well the model performs on the test set. preds_knn = predict(results_knn, wine_test) confusionMatrix(preds_knn, good_observed, positive=&#39;Good&#39;) Confusion Matrix and Statistics Reference Prediction Bad Good Bad 311 120 Good 165 702 Accuracy : 0.7804 95% CI : (0.7569, 0.8027) No Information Rate : 0.6333 P-Value [Acc &gt; NIR] : &lt; 2.2e-16 Kappa : 0.5177 Mcnemar&#39;s Test P-Value : 0.009152 Sensitivity : 0.8540 Specificity : 0.6534 Pos Pred Value : 0.8097 Neg Pred Value : 0.7216 Prevalence : 0.6333 Detection Rate : 0.5408 Detection Prevalence : 0.6680 Balanced Accuracy : 0.7537 &#39;Positive&#39; Class : Good Strengths &amp; Weaknesses Strengths41 Intuitive approach. Robust to outliers on the predictors. Weaknesses Susceptible to irrelevant features. Susceptible to correlated inputs. Ability to handle data of mixed types. Notably imbalanced target labels. Big data. Though approaches are available that help in this regard. Final Thoughts To be perfectly honest, knn approaches have mostly fallen out of favor, due to the above issues and their relatively poor predictive capability in common settings. However, they are conceptually simple, and thus a good way to start getting used to machine learning approaches, as knn regression doesn’t take one too far afield from what they already know (i.e. regression, distance matrix/clustering). Neural Networks Neural networks have been around for a long while as a general concept in artificial intelligence and even as a machine learning algorithm, and often work quite well. In some sense, neural networks can be thought of as nonlinear regression42. Visually however, we can see them as a graphical model with layers of inputs and outputs. Weighted combinations of the inputs are created and put through some function (e.g. the sigmoid function) to produce the next layer of inputs. This next layer goes through the same process to produce either another layer, or to predict the output, or even multiple outputs, which serves as the final layer. All the layers between the input and output are usually referred to as hidden layers. If there were a single hidden layer with a single unit and no transformation, then it becomes the standard regression problem. One of the issues with neural nets is determining how many hidden layers and how many hidden units in a layer. Overly complex neural nets will suffer from high variance will thus be less generalizable, particularly if there is less relevant information in the training data. Another notion is that of weight decay, however this tied to the same concept as regularization, which we discussed in a previous section, where a penalty term would be applied to a norm of the weights. Another regularization technique is dropout, where either observed or hidden units are randomly removed, and results averaged from the subsequent models. A comment about the following: if you are not set up for utilizing multiple processors the following will be relatively slow. You can replace method = 'avNNet', from the caret package, with method = 'nnet' from the nnet package, and shorten tuneLength = 3, which will be faster without much loss of accuracy. Also, the function we’re using has only one hidden layer, but the other neural net methods accessible via the caret package may allow for more, though the gains in prediction with additional layers are likely to be modest relative to complexity and computational cost. In addition, if the underlying function43 has additional arguments, you may pass those on in the train function itself. Here I am increasing the maxit, or maximum iterations, argument. results_nnet = train(good~., data=wine_train, method=&#39;avNNet&#39;, trControl=cv_opts, preProcess=c(&#39;center&#39;, &#39;scale&#39;), tuneLength=5, trace=F, maxit=1000) results_nnet Model Averaged Neural Network 5199 samples 9 predictor 2 classes: &#39;Bad&#39;, &#39;Good&#39; Pre-processing: centered (9), scaled (9) Resampling: Cross-Validated (10 fold) Summary of sample sizes: 4680, 4679, 4679, 4679, 4679, 4679, ... Resampling results across tuning parameters: size decay Accuracy Kappa 1 0e+00 0.7418758 0.4281475 1 1e-04 0.7418758 0.4281475 1 1e-03 0.7422604 0.4291346 1 1e-02 0.7420674 0.4290125 1 1e-01 0.7409135 0.4272660 3 0e+00 0.7564883 0.4601712 3 1e-04 0.7561070 0.4607032 3 1e-03 0.7582201 0.4652806 3 1e-02 0.7564905 0.4611899 3 1e-01 0.7593781 0.4670304 5 0e+00 0.7674557 0.4825182 5 1e-04 0.7622590 0.4742432 5 1e-03 0.7634147 0.4768157 5 1e-02 0.7620711 0.4743422 5 1e-01 0.7609150 0.4715354 7 0e+00 0.7714979 0.4965133 7 1e-04 0.7688004 0.4907053 7 1e-03 0.7637997 0.4782126 7 1e-02 0.7678400 0.4885388 7 1e-01 0.7607183 0.4712326 9 0e+00 0.7620574 0.4608267 9 1e-04 0.7755352 0.5048095 9 1e-03 0.7695685 0.4919259 9 1e-02 0.7701484 0.4935035 9 1e-01 0.7632264 0.4786383 Tuning parameter &#39;bag&#39; was held constant at a value of FALSE Accuracy was used to select the optimal model using the largest value. The final values used for the model were size = 9, decay = 1e-04 and bag = FALSE. We see that the best model has 9 hidden layer nodes and a decay parameter of 10^{-4}. Here is a visual display of those results. ggplot(results_nnet) ggplot(results_nnet) + theme_trueMinimal() + labs(x=&#39;Number of Hidden Units&#39;) + scale_x_continuous(breaks = c(1,3,5,7,9)) Typically, you might think of how many hidden units you want to examine in terms of the amount of data you have (i.e. estimated parameters to N ratio), and here we have a decent amount of data. In this situation, you might start with very broad values for the number of inputs (e.g. a sequence by 10s) and then narrow your focus (e.g. between 20 and 30s)44, but with at least some weight decay you should be able to avoid overfitting. I was able to get an increase in test accuracy of about 1.5% using up to 50 hidden units. preds_nnet = predict(results_nnet, wine_test) confusionMatrix(preds_nnet, good_observed, positive=&#39;Good&#39;) Confusion Matrix and Statistics Reference Prediction Bad Good Bad 326 112 Good 150 710 Accuracy : 0.7982 95% CI : (0.7753, 0.8197) No Information Rate : 0.6333 P-Value [Acc &gt; NIR] : &lt; 2e-16 Kappa : 0.558 Mcnemar&#39;s Test P-Value : 0.02226 Sensitivity : 0.8637 Specificity : 0.6849 Pos Pred Value : 0.8256 Neg Pred Value : 0.7443 Prevalence : 0.6333 Detection Rate : 0.5470 Detection Prevalence : 0.6626 Balanced Accuracy : 0.7743 &#39;Positive&#39; Class : Good We note improved prediction with the neural net model relative to the previous approaches, with increases in accuracy (79.8%), sensitivity, specificity etc. Strengths &amp; Weaknesses Strengths Good prediction generally. Incorporating the predictive power of different combinations of inputs. Some tolerance to correlated inputs. Weaknesses Susceptible to irrelevant features. Not robust to outliers. For traditional implementations, big data with complex models. Final Thoughts As a final note, to say that neural networks have undergone a resurgence over the past 10 years is a gross understatement. Practically everything you’re hearing about artificial intelligence and deep learning can be translated to ‘complicated neural networks’. Neural networks are so ubiquitous, the inevitable backlash has now begun, where some are questioning whether they can be taken any further. In any event, you should be familiar with them. A nice visualization of how neural networks work can be found at Chris Colah’s blog. I talk a little about deep learning in the next section, and there is some demo code in the appendix. Trees &amp; Forests Classification and regression trees provide yet another and notably different approach to prediction. Consider a single input variable and binary dependent variable. We will start by searching all values of the input to find a point where, if we partition the data at that point, it will lead to the best classification accuracy. So, for a single variable whose range might be 1 to 10, we find that a cut at 5.75 results in the best classification if all observations greater than or equal to 5.75 are classified as positive, and the rest negative. This general approach is fairly straightforward and conceptually easy to grasp, and it is because of this that tree approaches are appealing. Now let’s add a second input, also on a 1 to 10 range. We now might find that even better classification results if, upon looking at the portion of data regarding those greater than or equal to 5.75, that we only classify positive if they are also less than 3 on the second variable. The following is a hypothetical tree reflecting this. Now let’s see what kind of tree we might come up with for our data. The example tree here is based on the wine training data set. It is interpreted as follows. If the alcohol content is greater than 10.63%, a wine is classified as good. For those less than 10.63, if its volatile acidity is also less than .25, they are also classified as good, and of the remaining observations, if they are at least more than 9.85% in alcohol content (i.e. volatility &gt;.25, alcohol between 9.85 and 10.63), they also get classified as good. Any remaining observations are classified as bad wines. In this way, the classification tree is getting at interactions among the variables involved- volatile acidity interacts with alcohol in predicting whether the wine is good or not. Unfortunately, a single tree, while highly interpretable, does poorly for predictive purposes. In standard situations, we will instead use the power of many trees, i.e. a random forest, based on repeated sampling of the original data. So if we create 1000 new training data sets based on random samples of the original data (each of size N, i.e. a bootstrap of the original data set), we can run a tree for each, and assess the predictions each tree would produce for the observations for a hold-out set (or simply those observations which weren’t selected during the sampling process, the ‘out-of-bag’ sample), in which the new data is ‘run down the tree’ to obtain predictions. The final class prediction for an observation is determined by majority vote across all trees. Random forests are referred to as an ensemble method, one that is actually a combination of many models, and there are others we’ll mention later. There are other things to consider, such as how many variables to make available for consideration at each split, and this is the tuning parameter of consequence here (the argument mtry). In this case we will investigate subsets of 2 through 6 possible predictors. With this value determined via cross-validation, we can apply the best approach to the hold out test data set. There’s a lot going on here to be sure: there is a sampling process for cross-validation, there is re-sampling to produce the forest, there is random selection of mtry predictor variables, etc. But in the end, we are just harnessing the power of many trees, any one of which would be highly interpretable. In the following, the randomForest package does the work. rf_opts = data.frame(mtry=c(2:6)) results_rf = train(good~., data = wine_train, method = &#39;rf&#39;, preProcess = c(&#39;center&#39;, &#39;scale&#39;), trControl = cv_opts, tuneGrid = rf_opts, localImp = T, ntree=1000) Random Forest 5199 samples 9 predictor 2 classes: &#39;Bad&#39;, &#39;Good&#39; Pre-processing: centered (9), scaled (9) Resampling: Cross-Validated (10 fold) Summary of sample sizes: 4680, 4679, 4679, 4679, 4679, 4679, ... Resampling results across tuning parameters: mtry Accuracy Kappa 2 0.8315083 0.6287923 3 0.8297761 0.6259698 4 0.8286218 0.6237217 5 0.8278526 0.6227583 6 0.8251581 0.6168325 Accuracy was used to select the optimal model using the largest value. The final value used for the model was mtry = 2. The initial results look promising with mtry = 2 producing the best initial result. Now for application to the test set. preds_rf = predict(results_rf, wine_test) confusionMatrix(preds_rf, good_observed, positive=&#39;Good&#39;) Confusion Matrix and Statistics Reference Prediction Bad Good Bad 441 15 Good 35 807 Accuracy : 0.9615 95% CI : (0.9495, 0.9713) No Information Rate : 0.6333 P-Value [Acc &gt; NIR] : &lt; 2e-16 Kappa : 0.9163 Mcnemar&#39;s Test P-Value : 0.00721 Sensitivity : 0.9818 Specificity : 0.9265 Pos Pred Value : 0.9584 Neg Pred Value : 0.9671 Prevalence : 0.6333 Detection Rate : 0.6217 Detection Prevalence : 0.6487 Balanced Accuracy : 0.9541 &#39;Positive&#39; Class : Good This is our best result so far with 96.1% accuracy, with a lower bound well beyond the 63% we’d have guessing. Random forests do not suffer from some of the data-specific issues that might be influencing the other approaches, such as irrelevant and correlated predictors, and furthermore benefit from the combined information of many models. Such performance increases are not a given, but random forests are generally a good method to consider given their flexibility, and will often by default do very well relative to other approaches45. Incidentally, the underlying randomForest function here allows one to assess variable importance in a different manner46, and there are other functions used by caret that can produce their own metrics also. In this case, randomForest can provide importance based on a version of the ‘decrease in inaccuracy’ approach we talked before (as well as another index known as Gini impurity). The same two predictors are found to be most important and notably more than others- alcohol and volatile.acidity. Understanding the Results As we get further into black box models, it becomes harder to understand what is going on with the predictors. If our only goal is prediction, we might not care that much. However, advances continue to be made in helping us understand what’s happening under the hood. In what follows we seek to understand exactly what is going on with this model. Some of what will follow is specific to forest/tree approaches, but others are more general. Variable Importance VIMP To begin, we can get variable importance metrics as we did before with the regularized regression. How might we get such a measure from a random forest? The package itself provides two different ways. From the help file: The first measure is computed from permuting OOB (out-of-bag) data: For each tree, the prediction error on the out-of-bag portion of the data is recorded (error rate for classification, MSE for regression). Then the same is done after permuting each predictor variable (adding noise). The difference between the two are then averaged over all trees, and normalized by the standard deviation of the differences. The second measure is the total decrease in node impurities from splitting on the variable, averaged over all trees. For classification, the node impurity is measured by the Gini index. For regression, it is measured by residual sum of squares. So, the first compares the prediction of a variable with a random version of itself, while the second considers the error rates induced by splitting on a variable. Either way, we can then see which variables are relatively more important. The following shows the normalized result where the best predictor is given a value of 100, and the worst 0. varImp(results_rf) Variable Good alcohol 100.00 volatile.acidity 47.36 sulphates 19.16 residual.sugar 5.34 pH 4.16 total.sulfur.dioxide 4.12 chlorides 3.90 citric.acid 3.17 fixed.acidity 0.00 Minimal depth There are some issues with the basic variable importance approach outlined previously, such as being dependent on the type of prediction error used, and going much further than simply rank ordering variables. Minimal depth provides a more formal assessment based on the tree structure. Given the trees in a forest, more important variables are those that are split higher up the tree. We can essentially get the average depth at which a variable occurs across all trees. The first thing considered are the subtrees containing splits on the variable in question. We want maximal subtrees where the variable is split. For example, in our previous example the maximal subtree for alcohol is the entire tree, and its minimal depth is 0, as it is split at the root note47. For the following, we have two splits with alcohol in a single tree, thus there are two maximal subtrees one at depth 1 and one at depth 2. All maximal subtrees are considered for minimal depth importance. We can use randomForestExplainer to get and plot this information, both for individual variables but also interactions of two or more variables. Note that the latter will take an inordinately long time. library(randomForestExplainer) plot_min_depth_distribution(results_rf$finalModel) plot_min_depth_interactions(results_rf$finalModel, k=7) # only do 7 best interactions md_plot_inter$layers[[1]]$geom_params$width = .33 md_plot_inter$layers[[1]]$aes_params$alpha = .75 md_plot_inter$layers[[2]]$aes_params$colour = alpha(&#39;#ff5500&#39;, .5) # lollipop md_plot_inter$layers[[3]]$aes_params$colour = alpha(&#39;#ff5500&#39;, .25) md_plot_inter$layers[[3]]$aes_params$size = 1 # put the bar behind and let the alpha allow the marking md_plot_inter2 = md_plot_inter md_plot_inter2$layers[[3]] = md_plot_inter$layers[[1]] md_plot_inter2$layers[[1]] = md_plot_inter$layers[[3]] # md_plot_inter2 #+ # labs(y=&#39;Mean Minimal Depth&#39;, x = &#39;Interaction&#39;) #+ # viridis::scale_fill_viridis(end = .75, option = &#39;plasma&#39;, alpha=.5) #+ # alpha suddenly does nothing (see previous) # theme_trueMinimal() # + # theme(panel.border = element_blank(), # title = element_text(color=&#39;gray25&#39;)) We can see at least one difference relative to the previous VIMP measures. It appears that other measures’ importance are due more to their interaction with alcohol. Here is an interactive plot of the alcohol-volatile acidity interaction, which had the minimal depth. High alcohol and low acidity are very likely to be rated ‘good’, and vice versa. Other Measures There are still other measures of importance we can obtain, and the following shows how to get them and plot them using randomForestExplainer. Details about the measures can be found in the appendix. As one can see, they measure different things and thus will come to different conclusions. multi_imps = measure_importance(results_rf$finalModel) plot_importance_ggpairs(multi_imps) In the end, you’ll have a good idea of what predictor variables are driving your result. Effects Visualization So we can see that alcohol content and volatile acidity are the most important, but what is the nature of the relationship? Partial dependence plots allow us to understand the predictive effect of any particular covariate, controlling for the effects of others. This is particularly important in the case where we have nonlinear relationships as we do here. The ggRandomForests package provides such plots, as well as other ways to explore variable importance we’ll look at. Unfortunately, despite appearances to the contrary, I could find no example that used a basic randomForest class object, and none worked with the final model, so we’ll use the randomForestSRC package it is primarily associated with. In general though, you could get something similar by using the basic predict function and a little effort. # https://arxiv.org/pdf/1501.07196 # tibble causes problem so convert wine_train to standard df. library(ggRandomForests) rf2 = rfsrc(formula = good ~., data = data.frame(wine_train), mtry = results_rf$finalModel$mtry) gg_v = gg_variable(rf2) gg_md = gg_minimal_depth(rf2) # We want the top two ranked minimal depth variables only xvar = gg_md$topvars[1:2] plot(gg_v, xvar=xvar, panel=TRUE, partial=TRUE, alpha=.1) We can see with alcohol that the increasing probability of a good classification with increasing alcohol content only really starts with at least alcohol of 10% and only up until about 12.5% after which it’s not going to do much better. With volatile acidity, the trend is generally negative for increasing acidity. Note that you can think of these as edge-on views of the previous 3D image. LIME A relatively newer approach to understanding black box methods allows us to understand effects down to specific observations. The Local Interpretable Model-agnostic Explanations (LIME) approach is applicable to any model, and the basic idea is to provide a human interpretable prediction- not just a prediction, but also provide why that prediction was made. People using the model have some idea of what’s going on in a substantive sense (i.e. domain knowledge), but they will trust the prediction of such a model if they understand the reasoning behind it. Examining this prediction reasoning at representative or otherwise key settings of the predictors allows for a more global understanding of the model as a whole. Here is the gist of the approach (slightly more detail in [appendix][local-interpretable-model-agnostic-explanations]): Permute the data \\(n\\) times to create data with similar distributional properties to the original. Get similarity scores of the permuted observations to the observations you wish to explain. Make predictions with the permuted data based on the ML model. Select \\(m\\) features (e.g. forward selection, lasso) best describing the complex model outcome from the permuted data. Fit a simple model, e.g. standard regression, predicting the predictions from the ML model with the \\(m\\) features, where observations are weighted by similarity to the to-be-explained observations. Perhaps if we look at the product we can get more information. We get some sample observations, create an ‘explainer’, and then explain the predictions of those observations. set.seed(1234) sample_index = sample(1:nrow(wine_test), 5) sample_test = wine_test %&gt;% slice(sample_index) %&gt;% select(-good) library(lime) rf_lime = lime(wine_train, results_rf) rf_explain = explain(sample_test, rf_lime, n_features = 3, feature_select = &#39;highest_weights&#39;, labels = &#39;Good&#39;) The rf_explain object is a data frame that can be explored as is, or visualized. We’ll start with the feature plot. plot_features(rf_explain) Consider case 1. The predicted probability of the being a good wine is low at 0.98. The main reasons for this are that its citric acid is greater than .4, and its alcohol content is relatively low48. It’s chlorides content is actually working towards a ‘good’ classification. The weights give a us a good idea of how much the predicted probability would change if we did not have that particular feature present. The explanation fit is a measure of the fit of the simple model, basically just an R2. Now for case 2. This is very strongly classified as a good wine, with high alcohol and low acidity. Without those, this would be a much less confident understanding of the prediction. Case three has a similar expected probability of being classified ‘good’, but the value of that observation for volatile acidity actually makes us a little less confident. With case 4 &amp; 5, we see an interesting situation. Case 4 has a lower probability of being good, but the explanation is relatively better. The alcohol works against a good classification in both cases. For the Case 4 though, the chlorides works for it almost as much. Not so for Case 5. The following plot shows the same thing albeit more succinctly. plot_explanations(rf_explain) I don’t use lime with the other models because the story is the pretty much the same. Now that you know what to expect, feel free to try it out yourself. Strengths &amp; Weaknesses Strengths A single tree is highly interpretable. Easily incorporates features of different types (categorical and numeric). Tolerance to irrelevant features. Some tolerance to correlated inputs. Good with big data. Handling of missing values. Weaknesses Few, but like all techniques, it might be relatively less predictive in certain situations. Final Thoughts Random forests (and similar ensemble tree methods like boosting) are one of the better off-the-shelf classifiers you’ll come across. They handle a variety of the most common data scenarios49, are relatively fast, allow for an assessment of variable importance, and just plain work. Unless you have massive amounts of data, even all the complicated neural net approaches out there will struggle to do better50 in many common data situations without far more effort. Support Vector Machines Support Vector Machines (SVM) will be our last example, and are perhaps the least intuitive. SVMs seek to map the input space to a higher dimension via a kernel function, and in that transformed feature space, find a hyperplane that will result in maximal separation of the data. To better understand this process, consider the following image of two inputs, \\(x\\) and \\(y\\). Cursory inspection shows no easy separation between classes. However, if we can map the data to a higher dimension51, as shown in the following graph, we might find a more clear separation. Note that there are a number of choices in regard to the kernel function that does the mapping, but in that higher dimension, the decision boundary is chosen which will result in maximum distance (largest margin) between classes. Real data will not be so clean cut, and total separation impossible, but the idea is the same. For the following we use method = 'svmLinear2', where the underlying package is e1071. The tuning parameter is a regularization parameter applied to the cost function. results_svm = train(good~., data=wine_train, method=&#39;svmLinear2&#39;, preProcess=c(&#39;center&#39;, &#39;scale&#39;), trControl=cv_opts, tuneLength=5, probability=TRUE) # to get probs along with classifications Support Vector Machines with Linear Kernel 5199 samples 9 predictor 2 classes: &#39;Bad&#39;, &#39;Good&#39; Pre-processing: centered (9), scaled (9) Resampling: Cross-Validated (10 fold) Summary of sample sizes: 4680, 4679, 4679, 4679, 4679, 4679, ... Resampling results across tuning parameters: cost Accuracy Kappa 0.25 0.7445666 0.4272628 0.50 0.7449513 0.4283748 1.00 0.7445666 0.4276426 2.00 0.7449513 0.4285039 4.00 0.7451436 0.4290000 Accuracy was used to select the optimal model using the largest value. The final value used for the model was cost = 4. preds_svm = predict(results_svm, wine_test) confusionMatrix(preds_svm, good_observed, positive=&#39;Good&#39;) Confusion Matrix and Statistics Reference Prediction Bad Good Bad 277 117 Good 199 705 Accuracy : 0.7565 95% CI : (0.7323, 0.7797) No Information Rate : 0.6333 P-Value [Acc &gt; NIR] : &lt; 2.2e-16 Kappa : 0.4561 Mcnemar&#39;s Test P-Value : 5.199e-06 Sensitivity : 0.8577 Specificity : 0.5819 Pos Pred Value : 0.7799 Neg Pred Value : 0.7030 Prevalence : 0.6333 Detection Rate : 0.5431 Detection Prevalence : 0.6965 Balanced Accuracy : 0.7198 &#39;Positive&#39; Class : Good Results for the initial support vector machine do not match the random forest for this data set, with accuracy of 75.7%. However, you might choose a different kernel than the linear one used here, as well as tinker with other options. Strengths &amp; Weaknesses Strengths Good prediction in a variety of situations. Can utilize predictive power of linear combinations of inputs. Weaknesses Very black box. Computational scalability. Natural handling of mixed data types. Final Thoughts Support vector machines can provide very good prediction in a variety of settings, while doing relatively poorly in others. They will often require a notable amount of tuning, whereas generalizations of the SVM would estimate such parameters as part of the process. In short, you will typically have better, more interpretable, and/or less computationally intensive options in most circumstances. See, for example, the rescale function in the scales package.↩ For example, via principal components or partial least squares regression.↩ See Harrell (2015) for a good summary of reasons why not to.↩ The topic of ensembles is briefly noted later.↩ In situations where it is appropriate to calculate in the first place, model selection via AIC can often compare to the bootstrap and k-fold cross-validation approaches.↩ This term has always struck me as highly sub-optimal.↩ I think it would be interesting to have included characteristics of the people giving the rating.↩ Oddly, total.sulfur.dioxide is integer for all but a handful of values.↩ Some might recognize this approach. In traditional statistics settings, 1-R2 is known as tolerance, and its inverse is the variance inflation factor.↩ That alcohol content is a positive predictor of ‘quality’ is also seen with data I’ve scraped from BeerAdvocate. As such, if you are looking for a good drink via ratings, maybe temper the ratings a bit if the alcohol content is high.↩ Its now possible to use a model select the tuning parameters themselves, i.e. in which the parameters are outputs to the model, and the estimates would then be fed into the main model. Services such as Google’s CloudML already offer this, thus paving the way for data scientists to be fully automated well before truck drivers or fast food workers.↩ You can quickly obtain this plot via the dotplot function.↩ See the function dist in base R, for example.↩ Often referred to as unsupervised learning as there is no target/dependent variable. We do not cover such techniques here but they are a large part of machine learning just as they are with statistics in general.↩ See the knn.ani function in the animation package for a visual demonstration. Link to demo.↩ Note also you can just specify a tuning length instead. See the help file for the train function.↩ See table 10.1 in Hastie, Tibshirani, and Friedman (2009) for a more comprehensive list for this and other methods discussed in this section.↩ Neural nets, random forests, boosting, and additive models can all be put under the heading of basis function approaches.↩ For this example, ultimately the primary function is nnet in the nnet package that comes with base R.↩ You may find some rules of thumb from older sources, but using regularization and cross-validation is a much better way to ‘guess’.↩ In my opinion, random forests should be a baseline by which to judge new techniques. If one can’t consistently do better than an a standard random forest with default settings, there isn’t much to get excited about.↩ Our previous assessment was model independent.↩ Some might start with depth = 1.↩ I used lime’s default quantile binning approach which binarizes everything at their min, .25, .5, .75, and max quantile values. This makes for easier interpretation and comparison, but I also think that lime doesn’t evaluate the pre-processing argument of the model, and thus would be using the original data and weights that would reflect variable scales more than their importance.↩ A lot of papers testing various methods seem to avoid realistic data situations. One is never going to see data where every variable is normally or uniformly distributed, so it boggles a bit to think about why anyone would care if some technique does slightly better than a random forest under conditions that don’t exist naturally.↩ See Fernández-Delgado et al. (2014) for a recent assessment, but there have been others that come to similar conclusions.↩ Note that we regularly do this sort of thing in more mundane circumstances. For example, we map an \\(\\mathrm{N}\\ \\mathrm{x}\\ \\mathrm{p}\\) matrix to an \\(\\mathrm{N}\\ \\mathrm{x}\\ \\mathrm{N}\\) matrix when we compute a distance matrix for cluster analysis.↩ "],
["other.html", "Wrap-up Unsupervised Learning Ensembles Deep Learning Feature Selection &amp; Importance Natural Language Processing/Text Analysis Bayesian Approaches More Stuff Summary", " Wrap-up In this section, I note some other techniques one may come across, and others that will provide additional insight into machine learning applications. Unsupervised Learning Unsupervised learning generally speaking involves techniques in which we are utilizing unlabeled data. In this case we have our typical set of features we are interested in, but no particular response to map them to. In this situation, we are more interested in the discovery of structure within the data. For more examples, see this document. Clustering Many of the techniques used in unsupervised learning are commonly taught in various disciplines as simply “cluster” analysis. The gist is that we are seeking an unknown class structure rather than seeing how various inputs relate to a known class structure. Common techniques include k-means, hierarchical clustering, and model based approaches (e.g. mixture models). Latent Variable Models Sometimes the desire is to reduce the dimensionality of the inputs to a more manageable set of information. In this manner we are thinking that much of the data can be seen as having only a few sources of variability, often called latent variables or factors. Again, this takes familiar forms such as principal components and (“exploratory”) factor analysis, but would also include independence components analysis and partial least squares techniques. Note also that these can be part of a supervised technique (e.g. principal components regression) or the main focus of analysis (as with latent variable models in structural equation modeling). Graphical Structure Other techniques are available to understand structure among observations or features. Among the many approaches is the popular network analysis, where we can obtain links among observations and examine visually the structure of those data points, where observations are placed closer together that have closer ties to one another in some way. In still other situations, we aren’t so interested in the structure as we are in modeling the relationships and making predictions from the attributes of nodes. One can examine my document that covers more of these and latent variable approaches. An example network graph of U.S. senators in 2006. Node size is based on the betweeness centrality measure, edge size the percent agreement (graph filtered to edges &gt;= 65%). Color is based on the clustering discovered within the graph (link to data). Imputation We can also use ML techniques when we are missing data, as a means to impute the missing values. While many are familiar with this problem and standard techniques for dealing with it, it may not be obvious that ML techniques may also be used. For example, both k-nearest neighbors and random forest techniques have been applied to imputation52. Beyond this we can infer values that are otherwise unavailable in a different sense. Consider Netflix, Amazon and other sites that suggest various products based on what you already like or are interested in. In this case the suggested products have missing values for the user which are imputed or inferred based on their available data and other consumers similar to them who have rated the product in question. Such recommender systems are widely used these days. Ensembles In many situations we can combine the information of multiple models to enhance prediction. This can take place within a specific technique, e.g. random forests, or between models that utilize different techniques. I will discuss some standard techniques, but there are a great variety of forms in which model combination might take place. Bagging Bagging, or bootstrap aggregation, uses bootstrap sampling to create many data sets on which a procedure is then performed. The final prediction is based on an average of all the predictions made for each observation. In general, bagging helps reduce the variance while leaving bias unaffected. A conceptual outline of the procedure is provided. Model Generation Sample \\(N\\) observations with replacement \\(B\\) times to create \\(B\\) data sets of size \\(N\\). Apply the learning technique to each of \\(B\\) data sets to create \\(t\\) models. Store the \\(t\\) results. Classification For each of \\(t\\) number of models: Predict the class of \\(N\\) observations of the original data set. Return the class predicted most often across the \\(t\\) number of models (or alternatively, the proportion \\(t =1\\) as a probability). The approach would be identical for the continuous target domain, where the final prediction would be the average across all models. Boosting With boosting we take a different approach to refitting models. Consider a classification task in which we start with a basic learner and apply it to the data of interest. Next, the learner is refit, but with more weight (importance) given to misclassified observations. This process is repeated until some stopping rule is reached (e.g. reaching some \\(M\\) iterations). An example of the AdaBoost algorithm is provided (in the following \\(\\mathbb{I}\\) is the indicator function). Set initial weights \\(w_i\\) to \\(1/N\\). for \\(m=1:M\\) { Fit a classifier \\(m\\) with given weights to the data resulting in predictions \\(f^{(m)}_i\\) that minimizes some loss function. Compute the error rate \\(\\text{err}_m = \\frac{{\\sum_{i=1}^N}\\mathbb{I}(y_i\\ne f^{(m)}_i)}{\\sum^N_{i=1}w^{(m)}_i}\\) Compute \\(\\alpha_m = \\log[(1-err_m)/err_m]\\) Set \\(w_i \\leftarrow w_i\\exp[\\alpha_m \\mathbb{I}(y_i\\ne f^{(m)}_i)]\\) } Return \\(\\textrm{sign} [\\sum^M_{m=1}\\alpha_m f^{(m)}]\\) Boosting can be applied to a variety of tasks and loss functions, and in general is highly resistant to overfitting. A very popular implementation is XGBoost and its variants. The following shows an implementation. library(xgboost) modelLookup(&quot;xgbLinear&quot;) modelLookup(&quot;xgbTree&quot;) xgb_opts = expand.grid(eta=c(.3,.4), max_depth=c(9, 12), colsample_bytree=c(.6,.8), subsample=c(.5,.75,1), nrounds=1000, min_child_weight=1, gamma=0) set.seed(1234) results_xgb = train(good~., data=wine_train, method=&#39;xgbTree&#39;, preProcess=c(&#39;center&#39;, &#39;scale&#39;), trControl=cv_opts, tuneGrid=xgb_opts) results_xgb preds_gb = predict(results_xgb, wine_test) confusionMatrix(preds_gb, good_observed, positive=&#39;Good&#39;) Stacking Stacking is a method that can generalize beyond a single fitting technique, though it can be applied in a fashion similar to boosting for a single technique. Here we will use it broadly to mean any method to combine models of different forms. Consider the four approaches we demonstrated earlier: k-nearest neighbors, neural net, random forest, and the support vector machine. We saw that they do not have the same predictive accuracy, though they weren’t bad in general. Perhaps by combining their respective efforts, we could get even better prediction than using any particular one. The issue then is how we might combine them. We really don’t have to get too fancy with it, and can even use a simple voting scheme as in bagging. For each observation, note the predicted class on new data across models. The final prediction is the class that receives the most votes. Another approach would be to use a weighted vote, where the votes are weighted by the accuracy of their respective models. Another approach would use the predictions on the test set to create a data set of just the predicted probabilities from each learning scheme. We can then use this data to train a meta-learner using the test labels as the response. With the final meta-learner chosen, we then retrain the original models on the entire data set (i.e. including the test data). In this manner, the initial models and the meta-learner are trained separately and you get to eventually use the entire data set to train the original models. Now when new data becomes available, you feed them to the base level learners, get the predictions, and then feed the predictions to the meta-learner for the final prediction. Deep Learning Deep learning is all the rage these days, and for good reason- it keeps working and is highly flexible. Many techniques are largely focused on AI applications, but are not restricted to those. They’ve been employed successfully in a wide range of areas, e.g. facial recognition, computer vision, speech recognition, and natural language processing. Common techniques include deep feed forward neural networks53, convolutional neural networks, and recurrent/recursive neural networks. Armed with a basic knowledge of neural networks as presented earlier, you can see these as the next step. Such models require massive amounts of data, a lot of tuning, and generally serious hardware54. Python generally has the latest implementation of tools, through modules such as tensorflow, pytorch, and keras. There are tools in R, but they are wrappers for the Python modules, and the memory usage alone precludes standard R implementation, though packages like sparklyr and keras may eventually allow this. Because of the difficulty training such models, pre-trained models are often being applied in various situations. This can be a very dangerous situation if the data are not comparable. It’s fine to use a sentiment analysis model based on twitter feeds on other data from a review website. It’s another matter to use a model that was used for pedestrian detection in road data to look for tumors in x-rays. However, don’t be surprised if you see this stuff in drop-down menus for Excel in the not too distant future #whatcouldpossiblygowrong. You can start your journey into this sort of stuff at http://deeplearning.net/, and I have a minimal demos for R and Python in the appendix. Feature Selection &amp; Importance We hit on this topic some before, but much like there are a variety of ways to gauge performance, there are different approaches to select features and/or determine their importance. Invariably feature selection takes place from the outset when we choose what data to collect in the first place. Hopefully guided by theory, in other cases it may be restricted by user input, privacy issues, time constraints and so forth. But once we obtain the initial data set however, we may still want to trim the models under consideration. In standard approaches we might have in the past used forward or other selection procedure, or perhaps some more explicit model comparison approach. Concerning the content here, take for instance the lasso regularization procedure we spoke of earlier. ‘Less important’ variables may be shrunk entirely to zero, and thus feature selection is an inherent part of the process, and is useful in the face of many, many predictors, sometimes outnumbering our sample points. As another example, consider any particular approach where the importance metric might be something like the drop in accuracy when the variable is excluded. Variable importance was given almost full weight in the discussion of typical applied research in the past, based on statistical significance results from a one-shot analysis, and virtually ignorant of prediction on new data. We still have the ability to focus on feature performance with ML techniques, while shifting more of the focus toward prediction at the same time. For the uninitiated, it might require new ways of thinking about how one measures importance though. Natural Language Processing/Text Analysis In some situations, the data of interest is not in a typical matrix form but in the form of textual content, e.g. a corpus of documents (loosely defined). In this case, much of the work (like in most analyses but perhaps even more so) will be in the data preparation, as text is rarely if ever in a ready-to-analyze state. The eventual goals may include using the discovery of latent topics, parts-of-speech tagging, sentiment analysis, language identification, word usage in the prediction of an outcome, or examining the structure of the term usage graphically as in a network model. In addition, machine learning processes might be applied to sounds (acoustic data) to discern the speech characteristics and other information. Deep learning has been widely applied in this realm. For some applied examples of basic text analysis in R, see this. Bayesian Approaches It should be noted that the approaches outlined in this document are couched in the frequentist tradition. But one should be aware that many of the concepts and techniques would carry over into the Bayesian perspective, and even some machine learning techniques might only be feasible or make more sense within the Bayesian framework (e.g. online learning). However the core nature of Bayesian estimation makes it difficult to implement in ways that scale to even moderately large data situations. More Stuff Aside from what has already been noted, there still exists a great many applications for ML such as data set shift55, semi-supervised learning56, online learning57, and many more. Summary Cautionary Notes A standard mantra in machine learning and statistics generally is that there is no free lunch. All methods have certain assumptions, and if those don’t hold the results will be problematic at best. Also, even if in truth learner A is better than B, B can often outperform A in the finite situations we actually deal with in practice. In general, without context, no algorithm can be said to be any better than another on average. Furthermore, being more complicated doesn’t mean a technique is better. As previously noted, simply incorporating regularization and cross-validation goes a long way toward to improving standard techniques, and may perform quite well in many situations. The basic conclusion is that Machine learning is not magic! ML does not prove your theories, it does not make your data better, and the days of impressing someone simply because you’re using it have long since passed. Like any statistical technique, the reason to use ML is that is well-suited to the problem at hand. Some Guidelines Here are some thoughts to keep in mind, though these may be applicable to applied statistical practice generally. More data beats a cleverer algorithm, but a lot of data is not enough by itself (Domingos (2012)). Avoid overfitting. Let the data speak for itself. “Nothing is more practical than a good theory.”58 While getting used to ML, it might be best to start from simpler approaches and then work towards more black box ones that require more tuning. For example, regularized logistic regression \\(\\rightarrow\\) random forest \\(\\rightarrow\\) your-fancy-technique. Don’t get too excited if you aren’t doing significantly better than a random forest with default settings. Drawing up a visual diagram of your process is a good way to keep your analysis on the path to your goal. Some programs can even make this explicit. Keep the tuning parameter/feature selection process separate from the final test process for assessing error. Learn multiple models, selecting the best or possibly combining them. Conclusion It is hoped that this document sheds some light on a few areas that might otherwise be unfamiliar to some in more applied disciplines. The fields of statistics, computer science, engineering and related have rapidly evolved over the past couple decades. The tools available from them are myriad, and expanding all the time. Rather than feeling intimidated or overwhelmed, one should embrace the choice available, and have some fun with your data! References "],
["appendix.html", "Appendix Bias Variance Demo Programming Languages Local Interpretable Model-agnostic Explanations Various Variable Importance Measures Brief Glossary of Common Terms", " Appendix Bias Variance Demo set.seed(123) x = runif(1000) ytrue = sin(3*pi*x) basedat = cbind(x,ytrue)[order(x),] gendatfunc = function(noise=.5, n=1000){ x = runif(n) y = sin(3*pi*x) + rnorm(n, sd=noise) # truth d = cbind(x, y) d } gendat = replicate(100, gendatfunc(n=100)) str(gendat) library(kernlab) rbf1 = apply(gendat, 3, function(d) predict(gausspr(y~x, data=data.frame(d), kpar=list(sigma=.5)), newdata = data.frame(x), type=&#39;response&#39;)) rbf2 = apply(gendat, 3, function(d) predict(gausspr(y~x, data=data.frame(d)), newdata = data.frame(x), type=&#39;response&#39;) ) library(ggplot2); library(tidyverse); library(gridExtra) rbf1_samp = rbf1 %&gt;% data.frame %&gt;% cbind(x, .) %&gt;% slice(sample(1:100, 25)) %&gt;% gather(key=sample, value=yhat, -x) rbf2_samp = rbf2 %&gt;% data.frame %&gt;% cbind(x, .) %&gt;% slice(sample(1:100, 25)) %&gt;% gather(key=sample, value=yhat, -x) g1 = ggplot(data=data.frame(basedat)) + geom_blank() + geom_line(aes(x=x, y=yhat, group=sample), color=&#39;#ff5500&#39;, alpha=.25, data=rbf1_samp) + ylim(c(-1.5, 1.5)) + labs(y=&#39;&#39;, title=&#39;Low Variance&#39;) + lazerhawk::theme_trueMinimal() + theme( legend.key = ggplot2::element_rect(fill=&#39;#fffff8&#39;, colour = NA), legend.background = ggplot2::element_rect(fill=&#39;#fffff8&#39;, colour = NA), panel.background = ggplot2::element_blank(), panel.grid = ggplot2::element_blank(), strip.background = ggplot2::element_blank(), plot.background = ggplot2::element_rect(fill = &quot;#fffff8&quot;, colour = NA) ) g2 = ggplot(data=data.frame(basedat)) + geom_line(aes(x=x, y=ytrue), color=&#39;#00aaff&#39;) + geom_line(aes(x=x, y=yhat), color=&#39;#ff5500&#39;, data.frame(yhat=rowMeans(rbf1))) + ylim(c(-1.5, 1.5)) + labs(y=&#39;&#39;, title=&#39;High Bias&#39;) + lazerhawk::theme_trueMinimal() + theme( legend.key = ggplot2::element_rect(fill=&#39;#fffff8&#39;, colour = NA), legend.background = ggplot2::element_rect(fill=&#39;#fffff8&#39;, colour = NA), panel.background = ggplot2::element_blank(), panel.grid = ggplot2::element_blank(), strip.background = ggplot2::element_blank(), plot.background = ggplot2::element_rect(fill = &quot;#fffff8&quot;, colour = NA) ) g3 = ggplot(data=data.frame(basedat)) + geom_blank() + geom_line(aes(x=x, y=yhat, group=sample), color=&#39;#ff5500&#39;, alpha=.25, data=rbf2_samp) + ylim(c(-1.5, 1.5)) + labs(y=&#39;&#39;, title=&#39;High Variance&#39;) + lazerhawk::theme_trueMinimal() + theme( legend.key = ggplot2::element_rect(fill=&#39;#fffff8&#39;, colour = NA), legend.background = ggplot2::element_rect(fill=&#39;#fffff8&#39;, colour = NA), panel.background = ggplot2::element_blank(), panel.grid = ggplot2::element_blank(), strip.background = ggplot2::element_blank(), plot.background = ggplot2::element_rect(fill = &quot;#fffff8&quot;, colour = NA) ) g4 = ggplot(data=data.frame(basedat)) + geom_line(aes(x=x, y=ytrue), color=&#39;#00aaff&#39;) + geom_line(aes(x=x, y=yhat), color=&#39;#ff5500&#39;, data.frame(yhat=rowMeans(rbf2))) + ylim(c(-1.5, 1.5)) + labs(y=&#39;&#39;, title=&#39;Low Bias&#39;) + lazerhawk::theme_trueMinimal() + theme( legend.key = ggplot2::element_rect(fill=&#39;#fffff8&#39;, colour = NA), legend.background = ggplot2::element_rect(fill=&#39;#fffff8&#39;, colour = NA), panel.background = ggplot2::element_blank(), panel.grid = ggplot2::element_blank(), strip.background = ggplot2::element_blank(), plot.background = ggplot2::element_rect(fill = &quot;#fffff8&quot;, colour = NA) ) grid.arrange(g1, g2, g3, g4, ncol=2) Programming Languages R Demonstrations for this document were done with R, and specifically the caret package. I would highly recommend using it for your own needs, as it makes a lot the ML process simpler, while providing access to whatever technique you want to use, even while it comes with the ability to use hundreds of approaches out of the box. Deep learning example The following is a deep learning example using the same data from the previous examples, using keras. The package is an R wrapper for the keras module in Python, which itself is a wrapper for tensorflow. For more on using TensorFlow with R, check out RStudio’s documentation. This is a sequential neural net with three layers. I also add some dropout at each layer, which, given the material presented, you can think of as a means of regularization to avoid overfitting59. Note that keras works a bit differently, in that its objects are mutable, hence the there is no reassignment of the model. This R implementation also provides some nice visualizations to keep your interest while it runs. This is just a starting point, so don’t expect any better performance than those shown previously, but it’ll give you something to play with. Note that there is plenty to learn, and such models require copious amounts of tuning to work well. library(keras) X_train = model.matrix(lm(good ~ . -1, data=wine_train)) %&gt;% scale() y_train = as.numeric(wine_train$good) - 1 X_test = model.matrix(lm(good ~ . -1, data=wine_test)) %&gt;% scale() y_test = as.numeric(wine_test$good) - 1 model = keras_model_sequential() model %&gt;% layer_dense(units = 256, activation = &#39;relu&#39;, input_shape = c(9)) %&gt;% layer_dropout(rate = 0.4) %&gt;% layer_dense(units = 128, activation = &#39;relu&#39;) %&gt;% layer_dropout(rate = 0.3) %&gt;% layer_dense(units = 64, activation = &#39;relu&#39;) %&gt;% layer_dropout(rate = 0.2) %&gt;% layer_dense(units = 1, activation = &#39;sigmoid&#39;) summary(model) model %&gt;% compile( optimizer = &#39;nadam&#39;, loss = &quot;binary_crossentropy&quot;, metrics = &quot;accuracy&quot; ) history = model %&gt;% fit(X_train, y_train, epochs = 10000, validation_split = 0.1, verbose = 1, view_metrics = 1) plot(history) # model %&gt;% evaluate(X_test, y_test) ypred = model %&gt;% predict_classes(X_test) caret::confusionMatrix(ypred, y_test, positive=&#39;1&#39;) Python If your data fits on your machine and/or your analysis time is less than a couple hours, R is hands down the easiest to use to go from data to document, including if that document is an interactive website. That said, R probably isn’t even the most popular ML tool, because in many situations we have a lot more data, or simply need the predictions without frills and as fast as possible60. As such Python is the de facto standard in such situations, and probably the most popular development environment for machine learning. One can start with the scikit-learn module, using it much in the same way as was demonstrated with caret. It will get you very far, but for some situations, you may need more heavy-duty options like tensorflow, pytorch, etc. import pandas as pd import numpy as np from sklearn.model_selection import train_test_split from sklearn.preprocessing import MinMaxScaler from sklearn.ensemble import RandomForestClassifier from sklearn.metrics import confusion_matrix, accuracy_score # from os import chdir # if desired # chdir(&#39;your_directory&#39;) # import data wine = pd.read_csv(&#39;data/wine.csv&#39;) # data preprocessing np.random.seed(1234) X = wine.drop([&#39;free.sulfur.dioxide&#39;, &#39;density&#39;, &#39;quality&#39;, &#39;color&#39;, &#39;white&#39;,&#39;good&#39;], axis=1) X = MinMaxScaler().fit_transform(X) # by default on 0, 1 scale y = wine[&#39;good&#39;] X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2) # train model rf = RandomForestClassifier(n_estimators=1000) rf_train = rf.fit(X_train, y_train) # get test predictions rf_predict = rf_train.predict(X_test) # create confusion matrix, and accuracy cm = confusion_matrix(y_test,rf_predict) cm_prob = cm / np.sum(cm) # as probs print(cm_prob) [[0.26692308 0.09076923] [0.07461538 0.56769231]] acc = accuracy_score(y_test, rf_predict) acc = pd.DataFrame(np.array([acc]), columns=[&#39;Accuracy&#39;]) print(acc) Accuracy 0 0.834615 Deep learning example Here is a bit of code to get you started with a scikit approach to deep learning. This is a ‘deep neural net’ with seven layers. I also add some ‘dropout’. This isn’t too viable a model as far as settings go, and won’t do any better than those shown previously61, but it’ll run fine on your machine so you can at least play with it. import tensorflow.contrib.learn as skflow from sklearn import metrics feats = skflow.infer_real_valued_columns_from_input(X_train) classifier_tf = skflow.DNNClassifier(feature_columns=feats, hidden_units=[50, 50, 50, 40, 30, 20, 10], dropout=.2, n_classes=2) classifier_tf.fit(X_train, y_train, steps=10000) predictions = list(classifier_tf.predict(X_test, as_iterable=True)) score = metrics.accuracy_score(y_test, predictions) print(&quot;Accuracy: %f&quot; % score) Other I wouldn’t recommend a proprietary tool when better open source tools are available, but I will say Matlab is also very popular in machine learning, and in specific areas like image processing. Julia has been coming along as well. Of course, for maximum speed people still prefer lower level languages like C++, Java, and Go, and many of the Python modules are interfaces to such lower-level libraries. And for whatever reason, people are reinventing ML wheels in languages like Javascript and others. See the awesome list for more. What I can’t recommend is a traditional statistics package like SPSS, SAS, or Stata. Not only did they miss this boat by well over a decade, their offerings are slim and less capable. It seems SAS is probably the only one that’s made serious effort here, and it has some audience in the business world due to its long entrenchment there. And you don’t have to take my word for it- here’s a comparison of trends at indeed.com. Local Interpretable Model-agnostic Explanations The general approach lime takes to achieving this goal is as follows: For each prediction to explain, permute the observations n times62. Let the complex model predict the outcome of all permuted observations. Calculate the distance from all permutations to the original observation. Convert the distance to a similarity score. Select m features best describing the complex model outcome from the permuted data. Fit a simple model to the permuted data, explaining the complex model outcome with the m features from the permuted data weighted by its similarity to the original observation. Extract the feature weights from the simple model and use these as explanations for the complex models local behavior. See Ribeiro, Singh, and Guestrin (2016) for details. Various Variable Importance Measures This is taken from the randomForestExplainer package vignette. For a given predictor variable \\(X_j\\) accuracy_decrease (classification) – mean decrease of prediction accuracy after \\(X_j\\) is permuted gini_decrease (classification) – mean decrease in the Gini index of node impurity (i.e. increase of node purity) by splits on \\(X_j\\) mse_increase (regression) – mean increase of mean squared error after \\(X_j\\) is permuted node_purity_increase (regression) – mean node purity increase by splits on \\(X_j\\) as measured by the decrease in sum of squares mean_minimal_depth – mean minimal depth calculated in one of three ways specified by the parameter mean_sample no_of_trees – total number of trees in which a split on \\(X_j\\) occurs no_of_nodes – total number of nodes that use \\(X_j\\) for splitting (it is usually equal to no_of_trees if trees are shallow) times_a_root – total number of trees in which \\(X_j\\) is used for splitting the root node (i.e., the whole sample is divided into two based on the value of \\(X_j\\)) p_value – p-value for the one-sided binomial test using the following distribution: $$\\textrm{Binom}(\\texttt{no_of_nodes}, P(\\textrm{node splits on } X_j)$$ where we calculate the probability of split on $X_j$ as if $X_j$ was uniformly drawn from the $r$ candidate variables $$P(\\textrm{node splits on } X_j)=P(X_j \\textrm{ is a candidate}) \\cdot P(X_j \\textrm{ is selected}) = \\frac{r}{p} \\cdot \\frac{1}{r} = \\frac{1}{p}$$ This test tells us whether the observed number of successes (number of nodes in which $X_j$ was used for splitting) exceeds the theoretical number of successes if they were random (i.e. following the binomial distribution given above). Brief Glossary of Common Terms bias: could mean the intercept (e.g. in neural nets), typically refers to the bias in bias-variance decomposition classifier: specific model or technique (i.e. function) that maps observations to classes confusion matrix: a table of predicted class membership vs. true class membership hypothesis: a specific model \\(h(x)\\) of all possible in the hypothesis space \\(\\mathcal{H}\\) input, feature, attribute: independent variable, explanatory variable, covariate, predictor variable, column instance, example: observation, row learning: model fitting machine learning: a form of statistics utilizing various algorithms with a goal to generalize to new data situations regularization, penalization, shrinkage: The process of adding a penalty to the size of coefficients, thus shrinking them towards zero but resulting in less overfitting (at an increase to bias) supervised: has a dependent variable target, label: dependent variable, response, the outcome of interest unsupervised: no dependent variable (or rather, only dependent variables); think clustering, PCA etc. weights: coefficients, parameters References "],
["references.html", "References", " References "]
]
